import asyncio
import json
import re
import argparse
import urllib
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import quote

## ==================== ä¿®å¤åçš„å¯¼å…¥é€»è¾‘ ====================
import sys
import os

# 1. ä¼˜å…ˆå°è¯•ç›´æ¥å¯¼å…¥ (æœåŠ¡å™¨å¹³é“ºæ¨¡å¼ / PYTHONPATH å·²è®¾ç½®æ¨¡å¼)
try:
    from crawler_base import BaseCrawler, MultiCrawlerManager
except ImportError:
    # 2. å°è¯•ä»èµ„æºåŒ…å¯¼å…¥ (æœ¬åœ°æ‰“åŒ… EXE æ¨¡å¼)
    try:
        from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
    except ImportError:
        # 3. æœ¬åœ°å¼€å‘æ¨¡å¼ (ç›¸å¯¹è·¯å¾„å…œåº•)
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        try:
            from crawler.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
        except ImportError:
            # æœ€åçš„å€”å¼ºï¼šæ·»åŠ å½“å‰ç›®å½•
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from crawler_base import BaseCrawler, MultiCrawlerManager
# =========================================================

# å°è¯•å¯¼å…¥åŸºç±»
try:
    from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
except ImportError:
    import sys

    # å¦‚æœåœ¨å­ç›®å½•ï¼Œå°è¯•æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager

# eBay åŸºç¡€é…ç½®
EBAY_SEARCH_BASE = "https://www.ebay.com/sch/i.html"


class EbayCrawler(BaseCrawler):
    def extract_products(self, html_content, keyword, page_num):
        """
        eBay ä¸“å± HTML è§£æé€»è¾‘
        """
        soup = BeautifulSoup(html_content, "html.parser")
        products = []

        # 1. å®šä½å®¹å™¨
        containers = soup.select("li.s-item")
        if not containers:
            containers = soup.find_all("li", class_=re.compile(r"s-item", re.I))
        # å…œåº•ç­–ç•¥
        if not containers:
            for elem in soup.select(".s-item__title"):
                parent = elem.find_parent("li")
                if parent and parent not in containers:
                    containers.append(parent)

        print(f"ğŸ” [Port {self.port}] è§£æé¡µé¢ (Page {page_num})ï¼Œæ‰¾åˆ°å®¹å™¨: {len(containers)} ä¸ª")

        for container in containers:
            try:
                # 2. æå–é“¾æ¥
                link_elem = container.select_one("a.s-item__link")
                if not link_elem:
                    link_elem = container.find("a", href=re.compile(r"ebay\.com/itm/"))

                href = (link_elem.get("href", "") or "").strip() if link_elem else ""

                # è¿‡æ»¤æ— æ•ˆé“¾æ¥
                if not href or "itm/" not in href: continue

                # æ ¼å¼åŒ–é“¾æ¥
                if href.startswith("//"):
                    href = "https:" + href
                elif href.startswith("/"):
                    href = "https://www.ebay.com" + href
                href = href.replace("&amp;", "&")

                # 3. æå–æ ‡é¢˜
                title_elem = container.select_one(".s-item__title")
                title = (title_elem.get_text(strip=True) or "").strip() if title_elem else ""

                # è¿‡æ»¤å¹¿å‘Š
                if title == "Shop on eBay": continue

                # 4. æå–ä»·æ ¼
                price_elem = container.select_one(".s-item__price")
                price = ""
                if price_elem:
                    price_text = price_elem.get_text(strip=True) or ""
                    # ç®€å•æ¸…æ´—ä»·æ ¼
                    price_match = re.search(r"[\d,]+\.?\d*", price_text.replace(",", ""))
                    if price_match:
                        price = price_match.group().replace(",", "")

                # 5. æå–å›¾ç‰‡
                img_elem = container.select_one(".s-item__image img")
                if not img_elem: img_elem = container.select_one(".s-item__img img")
                image = ""
                if img_elem:
                    image = (img_elem.get("src") or
                             img_elem.get("data-src") or
                             img_elem.get("data-imgurl") or "")
                    if image.startswith("//"): image = "https:" + image

                # 6. ç»„è£…æ•°æ® (åŒ…å« page å­—æ®µ)
                record = {
                    "title": title,
                    "price": price,
                    "image": image,
                    "link": href,
                    "keyword": keyword,
                    "platform": "ebay",
                    "page": page_num  # âœ… å…³é”®ï¼šå†™å…¥é¡µç ç”¨äºæ–­ç‚¹
                }

                if title or href:
                    products.append(record)

            except Exception:
                continue

        return products

    async def crawl(self, tasks, max_count, output_dir):
        """
        eBay ä¸»çˆ¬å–å¾ªç¯ (ç¿»é¡µé€»è¾‘)
        """
        # æŒ‰è¿›åº¦æ’åº
        tasks.sort(key=lambda x: x[1])

        try:
            # 1. å¯åŠ¨æµè§ˆå™¨ (ä½¿ç”¨ BaseCrawler çš„æ–¹æ³•)
            await self.init_browser()
            if not self.page: return

            # 2. éå†ä»»åŠ¡
            for keyword, start_page in tasks:
                print(f"\n{'=' * 40}\n[Port {self.port}] çˆ¬å–: {keyword} (ä¸Šæ¬¡æ–­ç‚¹: Page {start_page})\n{'=' * 40}")

                current_count = 0
                keyword_products = []

                # âœ… ç¿»é¡µé€»è¾‘ï¼šç›´æ¥ä»ä¸‹ä¸€é¡µå¼€å§‹
                page_num = start_page + 1

                # å¾ªç¯ç›´åˆ°è¾¾åˆ°æ•°é‡
                while current_count < max_count:
                    # æ„å»ºæœç´¢ URL
                    encoded_kw = urllib.parse.quote(keyword)
                    url = f"{EBAY_SEARCH_BASE}?_nkw={encoded_kw}&_sacat=0&_from=R40&_pgn={page_num}"

                    print(f"  ğŸŒ [Port {self.port}] è®¿é—®ç¬¬ {page_num} é¡µ... (æœ¬è½®å·²æŠ“: {current_count})")

                    try:
                        await self.page.goto(url)
                        try:
                            await self.page.wait_for_load_state('domcontentloaded', timeout=15000)
                        except:
                            pass

                        # ç®€å•æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
                        await asyncio.sleep(1)
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        await asyncio.sleep(2)

                        # æå–æ•°æ®
                        html = await self.page.content()
                        items = self.extract_products(html, keyword, page_num)

                        if not items:
                            print(f"  âš ï¸ [Port {self.port}] ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œç»“æŸå½“å‰å…³é”®è¯ã€‚")
                            break

                        keyword_products.extend(items)
                        current_count += len(items)
                        print(f"  âœ“ [Port {self.port}] æœ¬é¡µæå– {len(items)} æ¡")

                        # å‡†å¤‡ä¸‹ä¸€é¡µ
                        page_num += 1
                        await asyncio.sleep(2)

                    except Exception as e:
                        print(f"  âŒ [Port {self.port}] é¡µé¢å‡ºé”™: {e}")
                        break

                    # å®‰å…¨é˜ˆå€¼ï¼Œé˜²æ­¢æ— é™ç¿»é¡µ
                    if page_num > 50: break

                # 3. ä¿å­˜æ•°æ® (start_page ç”¨äºåˆå¹¶)
                if keyword_products:
                    self._save_data(keyword, keyword_products, start_page, output_dir)
                else:
                    print(f"âš ï¸ [Port {self.port}] {keyword} æœªæå–åˆ°æ–°æ•°æ®")

        except Exception as e:
            print(f"âŒ [Port {self.port}] è¿›ç¨‹é”™è¯¯: {e}")
        finally:
            await self.close()

    def _save_data(self, product_name, new_data, start_index, output_dir):
        """
        é€šç”¨ä¿å­˜é€»è¾‘ (æ ‡å‡†ç‰ˆ)
        """
        final_data = new_data
        files_to_remove = []
        safe_name = re.sub(r'[<>:"/\\|?*]', "_", product_name)[:50]

        if start_index > 0:
            print(f"\nğŸ”„ [Port {self.port}] æ£€æµ‹åˆ°ç»­ä¼  (Start: {start_index})ï¼Œåˆå¹¶æ—§æ–‡ä»¶...")
            try:
                from pathlib import Path
                data_path = Path(output_dir)
                candidate_files = []
                for f in data_path.glob(f'{safe_name}_products_*.json'):
                    candidate_files.append(f)
                candidate_files.sort(key=lambda x: x.name, reverse=True)

                if candidate_files:
                    latest_json = candidate_files[0]
                    with open(latest_json, 'r', encoding='utf-8') as f:
                        old_data = json.load(f)

                    if isinstance(old_data, list) and len(old_data) > 0:
                        # ç¿»é¡µæ¨¡å¼æ£€æµ‹
                        is_page_logic = 'page' in old_data[0]
                        if is_page_logic:
                            print(f"    ğŸ“„ [ç¿»é¡µæ¨¡å¼] ä¸Šæ¬¡è¿›åº¦ Page {start_index}ï¼Œè¿½åŠ æ•°æ®...")

                        final_data = old_data + new_data
                        print(f"    â• åˆå¹¶æˆåŠŸ: æ—§({len(old_data)}) + æ–°({len(new_data)}) = æ€»({len(final_data)})")

                        files_to_remove.append(latest_json)

            except Exception as e:
                print(f"    âŒ åˆå¹¶å¤±è´¥: {e}")

        # ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if not os.path.exists(output_dir): os.makedirs(output_dir)

        json_path = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ [Port {self.port}] JSON: {os.path.basename(json_path)}")




        # æ¸…ç†
        if files_to_remove:
            for f in files_to_remove:
                try:
                    os.remove(f)
                except:
                    pass


# ==================== æ ‡å‡†ä»»åŠ¡è·å–é€»è¾‘ ====================
def get_tasks_from_file(name_file, max_count, data_dir):
    """
    ä»»åŠ¡åˆå§‹åŒ–å‡½æ•° (æ ‡å‡†ç‰ˆ)
    """
    import json
    from pathlib import Path

    try:
        if not os.path.exists(name_file):
            print(f"âŒ ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨: {name_file}")
            return []
        with open(name_file, 'r', encoding='utf-8') as f:
            names = json.load(f)
        product_names = list(set(names))
    except Exception as e:
        print(f"âŒ è¯»å–ä»»åŠ¡å¤±è´¥: {e}")
        return []

    tasks_progress = {name: 0 for name in product_names}
    data_path = Path(data_dir)

    if data_path.exists():
        print(f"ğŸ” æ‰«æ {data_dir} æ–­ç‚¹...")
        for json_file in data_path.glob('*_products_*.json'):
            if json_file.name.startswith('all_products'): continue

            # æ–‡ä»¶åç®€å•åŒ¹é…
            match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
            if not match: continue

            p_safe_name = match.group(1)

            # åå‘æŸ¥æ‰¾åŸå
            target_task = None
            for name in product_names:
                if re.sub(r'[<>:"/\\|?*]', "_", name)[:50] == p_safe_name:
                    target_task = name
                    break

            if target_task and target_task in tasks_progress:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if data and isinstance(data, list) and len(data) > 0:
                        last_item = data[-1]
                        # ä¼˜å…ˆå– page (ç¿»é¡µæ¨¡å¼)ï¼Œå¦åˆ™å– index (æ»šåŠ¨æ¨¡å¼)
                        current = int(last_item.get('page', 0))
                        if not current:
                            current = int(last_item.get('index', len(data)))

                        if current > tasks_progress[target_task]:
                            tasks_progress[target_task] = current
                except:
                    pass

    final_tasks = []
    for name, progress in tasks_progress.items():
        if progress > 0:
            print(f"  ğŸ”„ æ¢å¤ä»»åŠ¡: {name} (ä»è¿›åº¦ {progress} ç»§ç»­)")
        final_tasks.append((name, progress))

    return sorted(final_tasks, key=lambda x: x[0])


# ==================== ä¸»å…¥å£ ====================
# å°†è¿™æ®µä»£ç å¤åˆ¶æ›¿æ¢ depop_crawler.py å’Œ ebay_crawler.py æœ€åº•éƒ¨çš„ if __name__ == "__main__": éƒ¨åˆ†
# æ³¨æ„ï¼šè¦æŠŠ crawler_class=... é‚£ä¸€è¡Œæ”¹æˆå¯¹åº”çš„ç±»åï¼ˆDepopCrawler æˆ– EbayCrawlerï¼‰

# ... (get_tasks_from_file å‡½æ•°ä¿æŒä¸å˜) ...

if __name__ == "__main__":
    # 1. å®šä¹‰å‘½ä»¤è¡Œå‚æ•° (ä¸ backend_final.py å®Œç¾å¯¹æ¥)
    parser = argparse.ArgumentParser(description="åˆ†å¸ƒå¼çˆ¬è™«èŠ‚ç‚¹")
    parser.add_argument("--workers", type=int, default=2, help="å¹¶å‘çª—å£æ•°")
    parser.add_argument("--base_port", type=int, default=9222, help="èµ·å§‹ç«¯å£")
    parser.add_argument("--max_count", type=int, default=100, help="çˆ¬å–æ•°é‡")
    parser.add_argument("--output_dir", type=str, required=True, help="æ•°æ®ä¿å­˜ç»å¯¹è·¯å¾„")
    parser.add_argument("--task_file", type=str, required=True, help="ä»»åŠ¡æ–‡ä»¶è·¯å¾„")

    # æ¥æ”¶é¢å¤–å‚æ•° (å¦‚ cookies_file)
    parser.add_argument("--cookies_file", type=str, default=None, help="Cookieæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«ä»»åŠ¡ (PID: {os.getpid()}):")
    print(f"   - Workers: {args.workers}")
    print(f"   - Target: {args.max_count}")
    print(f"   - Output: {args.output_dir}")
    print(f"   - Task File: {args.task_file}")
    print("=" * 60)

    # 2. è·å–ä»»åŠ¡
    all_tasks = get_tasks_from_file(args.task_file, args.max_count, args.output_dir)

    if all_tasks:
        print(f"ğŸ“¦ ä»»åŠ¡æ€»æ•°: {len(all_tasks)}")

        # 3. å¯åŠ¨ç®¡ç†å™¨
        # [!] è¯·ç¡®ä¿è¿™é‡Œçš„ç±»åæ˜¯å½“å‰æ–‡ä»¶çš„çˆ¬è™«ç±» (å¦‚ DepopCrawler, EbayCrawler)
        manager = MultiCrawlerManager(
            crawler_class=EbayCrawler,  # <--- ä¿®æ”¹è¿™é‡Œï¼ï¼ï¼
            base_port=args.base_port,
            workers=args.workers,
            cookies_file=args.cookies_file  # ä¼ é€’ cookie å‚æ•°
        )

        try:
            asyncio.run(manager.run(all_tasks, args.max_count, args.output_dir))
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·åœæ­¢")
    else:
        print("ğŸ‰ æ— å¾…å¤„ç†ä»»åŠ¡æˆ–ä»»åŠ¡æ–‡ä»¶ä¸ºç©º")