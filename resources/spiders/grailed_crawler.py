import asyncio
import json
import re
import argparse
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import quote

# ==================== ä¿®å¤åçš„å¯¼å…¥é€»è¾‘ ====================
import sys
import os

# 1. ä¼˜å…ˆå°è¯•ç›´æ¥å¯¼å…¥ (æœåŠ¡å™¨å¹³é“ºæ¨¡å¼ / PYTHONPATH å·²è®¾ç½®æ¨¡å¼)
try:
    from crawler_base import BaseCrawler, MultiCrawlerManager
except ImportError:
    # 2. å°è¯•ä»èµ„æºåŒ…å¯¼å…¥ (æœ¬åœ°æ‰“åŒ… EXE æ¨¡å¼)
    try:
        from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
    except ImportError:
        # 3. æœ¬åœ°å¼€å‘æ¨¡å¼ (ç›¸å¯¹è·¯å¾„å…œåº•)
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        try:
            from crawler.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
        except ImportError:
            # æœ€åçš„å€”å¼ºï¼šæ·»åŠ å½“å‰ç›®å½•
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from crawler_base import BaseCrawler, MultiCrawlerManager
# =========================================================

# å°è¯•å¯¼å…¥åŸºç±»
try:
    from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager
except ImportError:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from resources.spiders.crawler_base import BaseCrawler, MultiCrawlerManager

GRAILED_SHOP_BASE = "https://www.grailed.com/shop"

class GrailedCrawler(BaseCrawler):
    def extract_products(self, html_content, skip_count=0):
        """
        Grailed ä¸“å±è§£æé€»è¾‘ - åŸºäºç”¨æˆ·æä¾›çš„ HTML ç»“æ„ (UserItem_root)
        """
        soup = BeautifulSoup(html_content, "html.parser")
        products = []

        # 1. ç²¾å‡†å®šä½å•†å“å®¹å™¨
        # ä½ çš„ HTML: <div class="UserItem_root__8Q2R_ UserItemForFeed_feedItem__5i2tc">
        # æˆ‘ä»¬ä½¿ç”¨æ­£åˆ™åŒ¹é… "UserItem_root" æ¥å¿½ç•¥åé¢çš„éšæœºå­—ç¬¦
        containers = soup.find_all("div", class_=re.compile(r"UserItem_root"))

        # å…œåº•ï¼šå¦‚æœæ”¹ç‰ˆå¯¼è‡´æ‰¾ä¸åˆ°ï¼Œå°è¯•æ‰¾åŒ…å« listings é“¾æ¥çš„çˆ¶çº§
        if not containers:
            print(f"âš ï¸ [Port {self.port}] æœªæ‰¾åˆ° UserItem_rootï¼Œå°è¯•å…œåº•ç­–ç•¥...")
            links = soup.find_all('a', href=re.compile(r'/listings/'))
            seen_parents = set()
            for link in links:
                # åœ¨ä½ çš„ç»“æ„ä¸­ï¼Œ<a> æ ‡ç­¾å°±åœ¨ UserItem_root ä¸‹é¢
                parent = link.find_parent('div', class_=re.compile(r"feedItem"))
                if not parent: parent = link.parent

                if parent and parent not in seen_parents:
                    containers.append(parent)
                    seen_parents.add(parent)

        containers_to_process = containers[skip_count:]
        if not containers_to_process:
            return []

        print(f"ğŸ” [Port {self.port}] è§£ææ–°å¢æ•°æ®: {len(containers_to_process)} æ¡...")

        local_index = skip_count
        for container in containers_to_process:
            try:
                local_index += 1
                product = {'index': local_index}

                # --- 1. æå–é“¾æ¥ ---
                # HTML: <a href="/listings/..." ... class="UserItem_link__kgEWg">
                link_elem = container.find("a", href=re.compile(r"/listings/"))
                if not link_elem: continue

                href = link_elem.get("href", "")
                if href.startswith("/"):
                    href = "https://www.grailed.com" + href
                # æ¸…é™¤ tracking å‚æ•°
                product['link'] = href.split('?')[0]

                # --- 2. æå–å›¾ç‰‡ ---
                # HTML: <img ... srcset="...url 1x, ...url 2x">
                img_elem = container.find("img")
                image_url = ""
                if img_elem:
                    # ä¼˜å…ˆå– srcset é‡Œæœ€é«˜æ¸…çš„é‚£å¼ ï¼ˆé€šå¸¸åœ¨æœ€åï¼‰
                    srcset = img_elem.get("srcset", "")
                    if srcset:
                        # åˆ†å‰² 'url 1x, url 2x' -> å–æœ€åä¸€ä¸ª -> å– url éƒ¨åˆ†
                        image_url = srcset.split(",")[-1].strip().split(" ")[0]
                    else:
                        image_url = img_elem.get("src", "")
                product['image'] = image_url

                # --- 3. æå–ä»·æ ¼ ---
                # HTML: <span class="Money_root__uOwWV" data-testid="Current">$250</span>
                # è¿™æ˜¯æœ€å‡†çš„å®šä½æ–¹å¼
                price = "N/A"
                price_elem = container.select_one('[data-testid="Current"]')
                if price_elem:
                    price = price_elem.get_text(strip=True)
                else:
                    # å¤‡ç”¨ï¼šæš´åŠ›æ‰¾ $ ç¬¦å·
                    text_price = container.find(string=re.compile(r"\$"))
                    if text_price: price = text_price.strip()
                product['price'] = price

                # --- 4. æå–è¯¦æƒ… (Brand, Title, Size) ---
                # HTML: UserItem_designer__N8CxZ, UserItem_size__QTA9F, UserItem_title__riOTf
                # æˆ‘ä»¬ä½¿ç”¨ class*= æ¥åŒ¹é…ï¼Œå¿½ç•¥åé¢çš„éšæœºå“ˆå¸Œ

                designer = ""
                item_title = ""
                size = ""

                designer_elem = container.select_one('[class*="UserItem_designer"]')
                if designer_elem: designer = designer_elem.get_text(strip=True)

                title_elem = container.select_one('[class*="UserItem_title"]')
                if title_elem: item_title = title_elem.get_text(strip=True)

                size_elem = container.select_one('[class*="UserItem_size"]')
                if size_elem: size = size_elem.get_text(strip=True)

                # æ‹¼æ¥æˆä¸€ä¸ªäººç±»å¯è¯»çš„å®Œæ•´æ ‡é¢˜
                # ä¾‹: "Nike What The Kobe 8 â€œProtroâ€ (Size: 10)"
                full_title_parts = []
                if designer: full_title_parts.append(designer)
                if item_title: full_title_parts.append(item_title)

                full_title = " ".join(full_title_parts)
                if size:
                    full_title += f" (Size: {size})"

                # å¦‚æœå®åœ¨æ²¡æå–åˆ°ï¼Œå›é€€åˆ°å–å…¨éƒ¨æ–‡æœ¬
                if not full_title.strip():
                    full_title = container.get_text(separator=" ", strip=True)[:100]

                product['title'] = full_title

                # --- 5. è¡¥å……å¹³å°å­—æ®µ ---
                product['Platform'] = 'grailed'
                product['Category'] = 'Clothing'

                products.append(product)

            except Exception as e:
                # print(f"è§£æé”™è¯¯: {e}") # è°ƒè¯•æ—¶å¯æ‰“å¼€
                continue

        return products

    async def crawl(self, tasks, max_count, output_dir):
        """
        Grailed ä¸»çˆ¬å–å¾ªç¯
        """
        # æŒ‰è¿›åº¦æ’åº
        tasks.sort(key=lambda x: x[1])

        try:
            # 1. å¯åŠ¨æµè§ˆå™¨
            await self.init_browser()
            if not self.page: return

            # 2. éå†ä»»åŠ¡
            for keyword, start_index in tasks:
                print(f"\n{'='*40}\n[Port {self.port}] çˆ¬å–: {keyword} (Index {start_index})\n{'='*40}")

                url = f"{GRAILED_SHOP_BASE}?query={quote(keyword)}"

                try:
                    await self.page.goto(url, timeout=60000)
                    try:
                        await self.page.wait_for_load_state('networkidle', timeout=15000)
                    except: pass
                except Exception as e:
                    print(f"âŒ [Port {self.port}] é¡µé¢è·³è½¬å¤±è´¥: {e}")
                    continue

                # --- æ— é™æ»šåŠ¨é€»è¾‘ ---
                current_count = 0
                retry_count = 0

                while current_count < max_count:
                    # æ»šåŠ¨åˆ°åº•éƒ¨
                    await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    await asyncio.sleep(2)

                    # å®æ—¶è®¡ç®—å½“å‰é¡µé¢å·²åŠ è½½çš„å•†å“æ•°
                    # ä½¿ç”¨ä¸ extract_products ç›¸åŒçš„é€»è¾‘è®¡æ•°
                    item_count = await self.page.evaluate("""() => {
                        return document.querySelectorAll('div[class*="UserItem_root"]').length
                    }""")

                    # å¦‚æœ js è®¡æ•°å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨è®¡æ•°
                    if item_count == 0:
                         item_count = await self.page.evaluate("""() => {
                            return document.querySelectorAll('a[href*="/listings/"]').length
                        }""")

                    if item_count > current_count:
                        current_count = item_count
                        retry_count = 0
                        print(f"  ğŸ“‰ [Port {self.port}] æ»šåŠ¨åŠ è½½ä¸­... (å½“å‰: {current_count})", end='\r')
                    else:
                        retry_count += 1
                        print(f"  âš ï¸ [Port {self.port}] æ— æ–°å†…å®¹ ({retry_count}/5)...")
                        # å°è¯•å›æ»šè§¦å‘æ‡’åŠ è½½
                        await self.page.evaluate("window.scrollBy(0, -800)")
                        await asyncio.sleep(1)
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                        if retry_count >= 5:
                            print(f"  ğŸ›‘ [Port {self.port}] å·²è¾¾åº•éƒ¨ï¼Œåœæ­¢æ»šåŠ¨")
                            break

                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ»¡è¶³æ•°é‡è¦æ±‚ï¼ˆåŠ ä¸Šä¹‹å‰çš„è¿›åº¦ï¼‰
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ˜¯é‡æ–°è·‘çš„ï¼Œæ‰€ä»¥åªè¦å½“å‰é¡µé¢çš„æ•°é‡å¤Ÿäº†å°±è¡Œ
                    if current_count >= (max_count - start_index) + 20: # å¤šæŠ“ä¸€ç‚¹ä½™é‡
                        break

                    await asyncio.sleep(1)

                # --- æå–ä¸ä¿å­˜ ---
                print(f"\n[Port {self.port}] å¼€å§‹æå–æ•°æ®...")
                try:
                    html = await self.page.content()
                    data = self.extract_products(html, skip_count=start_index)

                    if data:
                        # æˆªæ–­åˆ°éœ€è¦çš„æ•°é‡
                        needed = max_count - start_index
                        if len(data) > needed:
                            data = data[:needed]

                        self._save_data(keyword, data, start_index, output_dir)
                    else:
                        print(f"  âš ï¸ [Port {self.port}] æœªæå–åˆ°æœ‰æ•ˆæ•°æ®")
                except Exception as e:
                    print(f"  âŒ [Port {self.port}] å¤„ç†å¤±è´¥: {e}")

        except Exception as e:
            print(f"âŒ [Port {self.port}] è¿›ç¨‹å´©æºƒ: {e}")
        finally:
            await self.close()

    def _save_data(self, product_name, new_data, start_index, output_dir):
        """
        ä¿å­˜é€»è¾‘ï¼šJSON + CSV (å¸¦BOMå¤´)
        """
        final_data = new_data
        files_to_remove = []
        safe_name = re.sub(r'[<>:"/\\|?*]', "_", product_name)[:50]

        # åˆå¹¶é€»è¾‘
        if start_index > 0:
            print(f"\nğŸ”„ [Port {self.port}] æ£€æµ‹åˆ°ç»­ä¼  (Start: {start_index})ï¼Œåˆå¹¶æ—§æ–‡ä»¶...")
            try:
                from pathlib import Path
                data_path = Path(output_dir)
                candidate_files = []
                for f in data_path.glob(f'{safe_name}_products_*.json'):
                    candidate_files.append(f)
                candidate_files.sort(key=lambda x: x.name, reverse=True)

                if candidate_files:
                    latest_json = candidate_files[0]
                    with open(latest_json, 'r', encoding='utf-8') as f:
                        old_data = json.load(f)

                    if isinstance(old_data, list) and len(old_data) > 0:
                        final_data = old_data + new_data
                        print(f"    â• åˆå¹¶æˆåŠŸ: æ—§({len(old_data)}) + æ–°({len(new_data)}) = æ€»({len(final_data)})")
                        files_to_remove.append(latest_json)


            except Exception as e:
                print(f"    âŒ åˆå¹¶å¤±è´¥: {e}")

        # ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if not os.path.exists(output_dir): os.makedirs(output_dir)

        # ä¿å­˜ JSON
        json_path = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.json")
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            print(f"  ğŸ’¾ [Port {self.port}] JSON: {os.path.basename(json_path)}")
        except Exception as e:
            print(f"  âŒ JSON ä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜ CSV


        # æ¸…ç†æ—§æ–‡ä»¶
        if files_to_remove:
            for f in files_to_remove:
                try: os.remove(f)
                except: pass

# ==================== æ ‡å‡†ä»»åŠ¡è·å–é€»è¾‘ ====================
def get_tasks_from_file(name_file, max_count, data_dir):
    import json
    from pathlib import Path
    try:
        if not os.path.exists(name_file):
            return []
        with open(name_file, 'r', encoding='utf-8') as f:
            names = json.load(f)
        product_names = list(set(names))
    except Exception:
        return []

    tasks_progress = {name: 0 for name in product_names}
    data_path = Path(data_dir)

    if data_path.exists():
        print(f"ğŸ” æ‰«æ {data_dir} æ–­ç‚¹...")
        for json_file in data_path.glob('*_products_*.json'):
            if json_file.name.startswith('all_products'): continue
            match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
            if not match: continue

            p_safe_name = match.group(1)
            target_task = None
            for name in product_names:
                if re.sub(r'[<>:"/\\|?*]', "_", name)[:50] == p_safe_name:
                    target_task = name
                    break

            if target_task and target_task in tasks_progress:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if data and isinstance(data, list) and len(data) > 0:
                        # Grailed æ˜¯æ»šåŠ¨é€»è¾‘ï¼Œå– index
                        current = int(data[-1].get('index', len(data)))
                        if current > tasks_progress[target_task]:
                            tasks_progress[target_task] = current
                except: pass

    final_tasks = []
    for name, progress in tasks_progress.items():
        if progress < max_count:
            if progress > 0:
                print(f"  ğŸ”„ æ¢å¤ä»»åŠ¡: {name} (ä» {progress} ç»§ç»­)")
            final_tasks.append((name, progress))

    return sorted(final_tasks, key=lambda x: x[0])

# ==================== ä¸»å…¥å£ ====================
# ... (get_tasks_from_file å‡½æ•°ä¿æŒä¸å˜) ...

if __name__ == "__main__":
    # 1. å®šä¹‰å‘½ä»¤è¡Œå‚æ•° (ä¸ backend_final.py å®Œç¾å¯¹æ¥)
    parser = argparse.ArgumentParser(description="åˆ†å¸ƒå¼çˆ¬è™«èŠ‚ç‚¹")
    parser.add_argument("--workers", type=int, default=2, help="å¹¶å‘çª—å£æ•°")
    parser.add_argument("--base_port", type=int, default=9222, help="èµ·å§‹ç«¯å£")
    parser.add_argument("--max_count", type=int, default=100, help="çˆ¬å–æ•°é‡")
    parser.add_argument("--output_dir", type=str, required=True, help="æ•°æ®ä¿å­˜ç»å¯¹è·¯å¾„")
    parser.add_argument("--task_file", type=str, required=True, help="ä»»åŠ¡æ–‡ä»¶è·¯å¾„")

    # æ¥æ”¶é¢å¤–å‚æ•° (å¦‚ cookies_file)
    parser.add_argument("--cookies_file", type=str, default=None, help="Cookieæ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    print(f"ğŸš€ å¯åŠ¨çˆ¬è™«ä»»åŠ¡ (PID: {os.getpid()}):")
    print(f"   - Workers: {args.workers}")
    print(f"   - Target: {args.max_count}")
    print(f"   - Output: {args.output_dir}")
    print(f"   - Task File: {args.task_file}")
    print("=" * 60)

    # 2. è·å–ä»»åŠ¡
    all_tasks = get_tasks_from_file(args.task_file, args.max_count, args.output_dir)

    if all_tasks:
        print(f"ğŸ“¦ ä»»åŠ¡æ€»æ•°: {len(all_tasks)}")

        # 3. å¯åŠ¨ç®¡ç†å™¨
        # [!] è¯·ç¡®ä¿è¿™é‡Œçš„ç±»åæ˜¯å½“å‰æ–‡ä»¶çš„çˆ¬è™«ç±» (å¦‚ DepopCrawler, EbayCrawler)
        manager = MultiCrawlerManager(
            crawler_class=GrailedCrawler,  # <--- ä¿®æ”¹è¿™é‡Œï¼ï¼ï¼
            base_port=args.base_port,
            workers=args.workers,
            cookies_file=args.cookies_file  # ä¼ é€’ cookie å‚æ•°
        )

        try:
            asyncio.run(manager.run(all_tasks, args.max_count, args.output_dir))
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·åœæ­¢")
    else:
        print("ğŸ‰ æ— å¾…å¤„ç†ä»»åŠ¡æˆ–ä»»åŠ¡æ–‡ä»¶ä¸ºç©º")
