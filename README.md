# DAY2.16
æ›´æ–°ä¸€æ¬¡ï¼Œä¸­å°å·²ç»æ­å»ºå®Œæ¯•ï¼Œç»„ç»‡æ¶æ„ä¸ºï¼š

CrawlerApp{crawler.exe,resources{backendï¼Œspidersï¼Œweb},_internal}
ç‚¹å‡»exeå³å¯ä½¿ç”¨ï¼Œå¦‚éœ€ä¿®æ”¹ï¼Œåªç”¨ä¿®æ”¹spidersä¸­å¯¹åº”å„ç½‘ç«™çš„çˆ¬è™«å³å¯










# crawler_union
ç›®å‰åº”è¯¥å®ç°æ‰€æœ‰çˆ¬è™«ä»£ç çš„ä¿å­˜/å•†å“åˆå§‹åŒ–é€»è¾‘ï¼š
 
 1.å¯¹äºå•†å“åˆå§‹åŒ–åˆ—è¡¨ï¼Œå»ºè®®ä½¿ç”¨ï¼š


    def get_tasks_from_file(name_file, max_count, data_dir):


    # 1. è¯»å–åŸå§‹ä»»åŠ¡åˆ—è¡¨
    try:
        if not os.path.exists(name_file):
            print(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶: {name_file}")
            return []
        with open(name_file, 'r', encoding='utf-8') as f:
            names = json.load(f)
        # å»é‡
        product_names = list(set(names))
    except Exception as e:
        print(f"âŒ è¯»å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
        return []

    # 2. æ‰«æç°æœ‰çš„ JSON æ–‡ä»¶ï¼Œè·å–è¿›åº¦
    tasks_progress = {name: 0 for name in product_names}
    data_path = Path(data_dir)

    if data_path.exists():
        print(f"ğŸ” æ­£åœ¨æ‰«æ {data_dir} ç›®å½•ä¸‹çš„æ–­ç‚¹ä¿¡æ¯...")
        for json_file in data_path.glob('*_products_*.json'):
            # æ’é™¤æ±‡æ€»æ–‡ä»¶
            if json_file.name.startswith('all_products'): continue

            # è§£ææ–‡ä»¶å: name_products_timestamp.json
            match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
            if not match: continue

            p_name = match.group(1)


            if p_name in tasks_progress:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    if data and isinstance(data, list):
                        # è·å–æœ€åä¸€æ¡æ•°æ®çš„ index ä½œä¸ºå½“å‰è¿›åº¦
                        # å‡è®¾æ¯æ¡æ•°æ®éƒ½æœ‰ 'index' å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆ—è¡¨é•¿åº¦
                        last_item = data[-1]

                        current  = int(last_item.get('page', 0))
                        if not current:
                            current = int(last_item.get('index', len(data)))

                        # æ›´æ–°æœ€å¤§è¿›åº¦ï¼ˆé˜²æ­¢æœ‰å¤šä¸ªæ—§æ–‡ä»¶ï¼Œå–æœ€å¤§çš„é‚£ä¸ªï¼‰
                        if current > tasks_progress[p_name]:
                            tasks_progress[p_name] = current
                except Exception as e:
                    print(f"  âš ï¸ è¯»å–æ–‡ä»¶ {json_file.name} å¤±è´¥: {e}")
                    continue

    # 3. ç”Ÿæˆæœ€ç»ˆä»»åŠ¡åˆ—è¡¨
    final_tasks = []
    for name, progress in tasks_progress.items():
        if progress < max_count:
            if progress > 0:
                print(f"  ğŸ”„ æ¢å¤ä»»åŠ¡: {name} (ä» {progress} å¼€å§‹)")
            final_tasks.append((name, progress))
        else:
            # print(f"  âœ… è·³è¿‡å·²å®Œæˆ: {name}") # å¯é€‰ï¼šæ‰“å°å·²å®Œæˆçš„ä»»åŠ¡
            pass

    # æŒ‰åç§°æ’åºï¼Œä¿è¯æ¯æ¬¡è¿è¡Œé¡ºåºä¸€è‡´
    return sorted(final_tasks, key=lambda x: x[0])

    å¯¹äºæ— é™æ»šåŠ¨ç¿»é¡µçš„ç½‘ç«™ï¼Œç›´æ¥ä½¿ç”¨å•†å“ç¼–å·indexä½œä¸ºæ–­ç‚¹
    å¯¹äºç¿»é¡µç½‘ç«™ï¼Œä½¿ç”¨pageä½œä¸ºæ–­ç‚¹


2.å¯¹äºå•†å“ä¿å­˜ï¼Œå»ºè®®ä½¿ç”¨ï¼š

        def _save_data(self, product_name, new_data, start_index, output_dir):

        """é€šç”¨ä¿å­˜æ•°æ®è¾…åŠ©å‡½æ•°ï¼šæ”¯æŒç´¢å¼•åˆå¹¶ä¸é¡µç åˆå¹¶"""
        
        final_data = new_data
        files_to_remove = []
        
        # 1. é¢„å¤„ç†æ–‡ä»¶åé€»è¾‘ï¼ˆeBayå…³é”®è¯å¯èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼Œéœ€ä¸ä»»åŠ¡è·å–é€»è¾‘ä¸€è‡´ï¼‰
        
        safe_name = re.sub(r'[<>:"/\\|?*]', "_", product_name)[:50]
        if start_index > 0:
            print(f"\nğŸ”„ [åˆå¹¶æ¨¡å¼] æ£€æµ‹åˆ°ç»­ä¼  (èµ·å§‹æ ‡è®° {start_index})ï¼Œæ£€ç´¢æ—§æ–‡ä»¶...")
            try:
                from pathlib import Path
                data_path = Path(output_dir)
                candidate_files = []
                
                # ä½¿ç”¨æ¸…æ´—åçš„æ–‡ä»¶åè¿›è¡Œæœç´¢
                
                for f in data_path.glob(f'{safe_name}_products_*.json'):
                    candidate_files.append(f)
                
                candidate_files.sort(key=lambda x: x.name, reverse=True)

                if candidate_files:
                    latest_json = candidate_files[0]
                    with open(latest_json, 'r', encoding='utf-8') as f:
                        old_data = json.load(f)

                    if isinstance(old_data, list) and len(old_data) > 0:
                        # --- æ ¸å¿ƒæ”¹è¿›ï¼šæ£€æµ‹é€»è¾‘ç±»å‹ ---
                        # æ£€æŸ¥ç¬¬ä¸€æ¡æ•°æ®æ˜¯å¦æœ‰ 'page' å­—æ®µ
                        is_page_logic = 'page' in old_data[0]

                        if is_page_logic:
                            # ç¿»é¡µé€»è¾‘ï¼šstart_index æ­¤æ—¶ä»£è¡¨çš„æ˜¯ max_page
                            # æˆ‘ä»¬ä¸åšé•¿åº¦å¼ºæ ¡éªŒï¼Œå› ä¸ºæ¯é¡µæ•°é‡å¯èƒ½ä¸å›ºå®š
                            print(f"    ğŸ“„ æ£€æµ‹åˆ° [ç¿»é¡µé€»è¾‘]ï¼Œä¸Šæ¬¡çˆ¬å–è‡³ç¬¬ {start_index} é¡µ")
                        else:
                            # æ— é™æ»šåŠ¨é€»è¾‘ï¼šæ ¡éªŒé•¿åº¦
                            if len(old_data) != start_index:
                                print(f"    âš ï¸ è­¦å‘Š: æ—§æ•°æ®é•¿åº¦ ({len(old_data)}) ä¸ start_index ({start_index}) ä¸ä¸€è‡´")
                        
                        # åˆå¹¶æ•°æ®
                        final_data = old_data + new_data
                        print(f"    â• åˆå¹¶æˆåŠŸ: æ—§({len(old_data)}) + æ–°({len(new_data)}) = æ€»({len(final_data)})")
                        
                        # è®°å½•å¾…åˆ é™¤æ—§æ–‡ä»¶
                        files_to_remove.append(latest_json)
                        old_csv = latest_json.with_suffix('.csv')
                        if old_csv.exists(): 
                            files_to_remove.append(old_csv)
                else:
                    print("    âš ï¸ æœªæ‰¾åˆ°æ—§æ–‡ä»¶ï¼Œå°†ä½œä¸ºæ–°ä»»åŠ¡ä¿å­˜")
            except Exception as e:
                print(f"    âŒ åˆå¹¶å‡ºé”™: {e}")

        # 2. æŒä¹…åŒ–æ–°æ•°æ®
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        new_json_name = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.json")
        new_csv_name = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.csv")

        if not os.path.exists(output_dir): 
            os.makedirs(output_dir)

        # ä¿å­˜ JSON
        with open(new_json_name, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONä¿å­˜: {os.path.basename(new_json_name)}")

        # ä¿å­˜ CSV
        import csv
        if final_data:
            with open(new_csv_name, 'w', encoding='utf-8', newline='') as f:
                keys = final_data[0].keys()
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(final_data)
            print(f"ğŸ’¾ CSVä¿å­˜: {os.path.basename(new_csv_name)}")

        # 3. æ¸…ç†é™ˆæ—§æ–‡ä»¶
        if files_to_remove:
            print(f"ğŸ§¹ æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶...")
            for f in files_to_remove:
                try:
                    os.remove(f)
                    print(f"    ğŸ—‘ï¸ åˆ é™¤: {f.name}")
                except Exception as e:
                    print(f"    âš ï¸ æ— æ³•åˆ é™¤æ—§æ–‡ä»¶ {f.name}: {e}")
        ç»Ÿä¸€ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥ç¡®ä¿åˆå¹¶æ•°æ®æ—¶çš„é²æ£’æ€§



  3.ä¼ å…¥é€»è¾‘ï¼š
  å¯¹äºæ»šåŠ¨å¼ç½‘ç«™ï¼Œè¾“å…¥åˆå§‹indexï¼Œæ¥è·³è¿‡å‰indexä¸ªæ•°æ®ï¼Œå¯¹äºç¿»é¡µå¼ï¼Œåˆ™è·³è¿‡å‰indexé¡µå¼€å§‹çˆ¬å–
