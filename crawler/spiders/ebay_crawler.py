import asyncio
import json
import os
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup

# é€‚é…å¯¼å…¥è·¯å¾„ï¼šå°è¯•ä»åŒçº§æˆ–çˆ¶çº§å¯¼å…¥ crawler_base
try:
    from crawler_base import BaseCrawler, MultiCrawlerManager
except ImportError:
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from crawler.crawler_base import BaseCrawler, MultiCrawlerManager

# eBay åŸºç¡€é…ç½®
EBAY_SEARCH_BASE = "https://www.ebay.com/sch/i.html"

class EbayCrawler(BaseCrawler):
    def extract_products(self, html_content, keyword, page_num):
        """
        eBay ä¸“å± HTML è§£æé€»è¾‘
        âœ… å…³é”®ç‚¹ï¼šå°† page_num å†™å…¥æ¯æ¡æ•°æ®ï¼Œç”¨äºç²¾å‡†æ–­ç‚¹ç»­ä¼ 
        """
        soup = BeautifulSoup(html_content, "html.parser")
        products = []
        
        # å®¹å™¨é€‰æ‹©å™¨ç­–ç•¥
        containers = soup.select("li.s-item")
        if not containers:
            containers = soup.find_all("li", class_=re.compile(r"s-item", re.I))
        # å…œåº•ç­–ç•¥
        if not containers:
            for elem in soup.select(".s-item__title"):
                parent = elem.find_parent("li")
                if parent and parent not in containers:
                    containers.append(parent)

        print(f"ğŸ” [Port {self.port}] è§£æé¡µé¢ (Page {page_num})ï¼Œæ‰¾åˆ°å®¹å™¨: {len(containers)} ä¸ª")

        for container in containers:
            try:
                # 1. æå–é“¾æ¥
                link_elem = container.select_one("a.s-item__link")
                if not link_elem:
                    link_elem = container.find("a", href=re.compile(r"ebay\.com/itm/"))
                href = (link_elem.get("href", "") or "").strip() if link_elem else ""
                
                # è¿‡æ»¤æ— æ•ˆé“¾æ¥
                if not href or "itm/" not in href: continue

                # æ ¼å¼åŒ–é“¾æ¥
                if href.startswith("//"): href = "https:" + href
                elif href.startswith("/"): href = "https://www.ebay.com" + href
                href = href.replace("&amp;", "&")

                # 2. æå–æ ‡é¢˜
                title_elem = container.select_one(".s-item__title")
                title = (title_elem.get_text(strip=True) or "").strip() if title_elem else ""
                
                # è¿‡æ»¤å¹¿å‘Š
                if title == "Shop on eBay": continue

                # 3. æå–ä»·æ ¼
                price_elem = container.select_one(".s-item__price")
                price = ""
                if price_elem:
                    price_text = price_elem.get_text(strip=True) or ""
                    price_match = re.search(r"[\d,]+\.?\d*", price_text.replace(",", ""))
                    if price_match:
                        price = price_match.group().replace(",", "")

                # 4. æå–å›¾ç‰‡
                img_elem = container.select_one(".s-item__image img")
                if not img_elem: img_elem = container.select_one(".s-item__img img")
                image = ""
                if img_elem:
                    image = (img_elem.get("src") or 
                             img_elem.get("data-src") or 
                             img_elem.get("data-imgurl") or "")
                    if image.startswith("//"): image = "https:" + image

                # ç»„è£…æ•°æ®
                record = {
                    "title": title,
                    "price": price,
                    "image": image,
                    "link": href,
                    "keyword": keyword, 
                    "platform": "ebay",
                    "page": page_num  # âœ… å¿…é¡»åŒ…å« page å­—æ®µ
                }
                
                if title or href:
                    products.append(record)

            except Exception:
                continue
                
        return products

    async def crawl(self, tasks, max_count, output_dir):
        """
        eBay çˆ¬å–ä¸»å¾ªç¯
        tasks æ ¼å¼: [(keyword, start_page), ...]
        start_page: ä¸Šæ¬¡çˆ¬å–çš„æœ€å¤§é¡µç 
        """
        # ç®€å•æ’åºä»»åŠ¡
        tasks.sort(key=lambda x: x[1])

        try:
            # 1. å¯åŠ¨æµè§ˆå™¨
            await self.init_browser()
            if not self.page: return

            # 2. éå†ä»»åŠ¡
            for keyword, start_page in tasks:
                # è¿™é‡Œçš„ max_count æŒ‡çš„æ˜¯ç›®æ ‡å•†å“æ¡æ•°
                print(f"\n{'='*40}\n[Port {self.port}] çˆ¬å–: {keyword} (ä¸Šæ¬¡æ–­ç‚¹: Page {start_page})\n{'='*40}")
                
                current_count = 0 
                keyword_products = []
                
                # âœ… å…³é”®é€»è¾‘ï¼šç›´æ¥ä»æ–­ç‚¹é¡µçš„ä¸‹ä¸€é¡µå¼€å§‹ï¼Œä¸ä½¿ç”¨ item count ä¼°ç®—
                page_num = start_page + 1
                
                # å¾ªç¯æ¡ä»¶ï¼šç›´åˆ°æŠ“å¤Ÿæ•°é‡æˆ–æ— æ•°æ®
                while current_count < max_count:
                    # æ„å»º URL
                    encoded_kw = urllib.parse.quote(keyword)
                    url = f"{EBAY_SEARCH_BASE}?_nkw={encoded_kw}&_sacat=0&_from=R40&_pgn={page_num}"
                    
                    print(f"  ğŸŒ [Port {self.port}] è®¿é—®ç¬¬ {page_num} é¡µ... (æœ¬è½®å·²æŠ“: {current_count})")
                    
                    try:
                        await self.page.goto(url)
                        try: await self.page.wait_for_load_state('domcontentloaded', timeout=15000)
                        except: pass
                        
                        # æ»šåŠ¨è§¦å‘æ‡’åŠ è½½
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
                        await asyncio.sleep(1)
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        await asyncio.sleep(2)

                        # æå–æ•°æ®
                        html = await self.page.content()
                        items = self.extract_products(html, keyword, page_num)
                        
                        if not items:
                            print(f"  âš ï¸ [Port {self.port}] ç¬¬ {page_num} é¡µæ— æ•°æ®ï¼Œç»“æŸå½“å‰å…³é”®è¯ã€‚")
                            break
                        
                        keyword_products.extend(items)
                        current_count += len(items)
                        print(f"  âœ“ [Port {self.port}] æå– {len(items)} æ¡")
                        
                        # ç¿»é¡µ
                        page_num += 1
                        await asyncio.sleep(2) # ç¤¼è²Œç­‰å¾…

                    except Exception as e:
                        print(f"  âŒ [Port {self.port}] é¡µé¢å‡ºé”™: {e}")
                        break
                    
                    # é˜²æ­¢æ— é™ç¿»é¡µçš„å®‰å…¨é˜ˆå€¼ (å¯é€‰)
                    if page_num > 100: break

                # 3. ä¿å­˜æ•°æ®
                if keyword_products:
                    # ä¼ å…¥ start_page ä½œä¸ºæ–­ç‚¹æ ‡è¯†ï¼Œ_save_data ä¼šå¤„ç†åˆå¹¶
                    self._save_data(keyword, keyword_products, start_page, output_dir)
                else:
                    print(f"âš ï¸ [Port {self.port}] {keyword} æœªæå–åˆ°æ–°æ•°æ®")

        except Exception as e:
            print(f"âŒ [Port {self.port}] è¿›ç¨‹é”™è¯¯: {e}")
        finally:
            await self.close()

    def _save_data(self, product_name, new_data, start_index, output_dir):
        """
        é€šç”¨ä¿å­˜é€»è¾‘ (ç¬¦åˆ README æ ‡å‡†)
        æ”¯æŒæ ¹æ® page å­—æ®µè‡ªåŠ¨åˆ¤æ–­ç¿»é¡µé€»è¾‘å¹¶åˆå¹¶æ•°æ®
        """
        final_data = new_data
        files_to_remove = []
        
        # æ–‡ä»¶åæ¸…æ´—
        safe_name = re.sub(r'[<>:"/\\|?*]', "_", product_name)[:50]
        
        if start_index > 0:
            print(f"\nğŸ”„ [Port {self.port}] æ£€æµ‹åˆ°ç»­ä¼  (Start: {start_index})ï¼Œåˆå¹¶æ—§æ–‡ä»¶...")
            try:
                from pathlib import Path
                data_path = Path(output_dir)
                candidate_files = []
                for f in data_path.glob(f'{safe_name}_products_*.json'):
                    candidate_files.append(f)
                candidate_files.sort(key=lambda x: x.name, reverse=True)

                if candidate_files:
                    latest_json = candidate_files[0]
                    with open(latest_json, 'r', encoding='utf-8') as f:
                        old_data = json.load(f)

                    if isinstance(old_data, list) and len(old_data) > 0:
                        # æ ¸å¿ƒæ£€æµ‹ï¼šæ˜¯ç¿»é¡µé€»è¾‘(eBay) è¿˜æ˜¯ æ»šåŠ¨é€»è¾‘(Depop)
                        is_page_logic = 'page' in old_data[0]

                        if is_page_logic:
                            # ç¿»é¡µé€»è¾‘ï¼šç›´æ¥è¿½åŠ æ•°æ®
                            print(f"    ğŸ“„ [ç¿»é¡µæ¨¡å¼] ä¸Šæ¬¡è¿›åº¦ Page {start_index}ï¼Œè¿½åŠ æ•°æ®...")
                        else:
                            # æ»šåŠ¨é€»è¾‘ï¼šæ£€æŸ¥é•¿åº¦
                            if len(old_data) != start_index:
                                print(f"    âš ï¸ é•¿åº¦æ ¡éªŒä¸ä¸€è‡´: æ—§({len(old_data)}) vs æ ‡è®°({start_index})")
                        
                        final_data = old_data + new_data
                        print(f"    â• åˆå¹¶æˆåŠŸ: æ—§({len(old_data)}) + æ–°({len(new_data)}) = æ€»({len(final_data)})")
                        
                        files_to_remove.append(latest_json)
                        old_csv = latest_json.with_suffix('.csv')
                        if old_csv.exists(): files_to_remove.append(old_csv)
            except Exception as e:
                print(f"    âŒ åˆå¹¶å¤±è´¥: {e}")

        # æŒä¹…åŒ–
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        if not os.path.exists(output_dir): os.makedirs(output_dir)
        
        json_path = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ [Port {self.port}] JSON: {os.path.basename(json_path)}")
        
        # CSV ä¿å­˜
        import csv
        if final_data:
            csv_path = os.path.join(output_dir, f"{safe_name}_products_{timestamp}.csv")
            try:
                with open(csv_path, 'w', encoding='utf-8', newline='') as f:
                    keys = final_data[0].keys()
                    writer = csv.DictWriter(f, fieldnames=keys)
                    writer.writeheader()
                    writer.writerows(final_data)
            except: pass

        # æ¸…ç†
        if files_to_remove:
            for f in files_to_remove:
                try: os.remove(f)
                except: pass

# ==================== æ ‡å‡†ä»»åŠ¡è·å–é€»è¾‘ ====================
def get_tasks_from_file(name_file, max_count, data_dir):
    """
    ç¬¦åˆ README æ ‡å‡†çš„ä»»åŠ¡åˆå§‹åŒ–å‡½æ•°
    è‡ªåŠ¨è¯†åˆ« page æ–­ç‚¹æˆ– index æ–­ç‚¹
    """
    import json
    from pathlib import Path

    try:
        if not os.path.exists(name_file):
            print(f"âŒ ä»»åŠ¡æ–‡ä»¶ä¸å­˜åœ¨: {name_file}")
            return []
        with open(name_file, 'r', encoding='utf-8') as f:
            names = json.load(f)
        product_names = list(set(names))
    except Exception as e:
        print(f"âŒ è¯»å–ä»»åŠ¡å¤±è´¥: {e}")
        return []

    tasks_progress = {name: 0 for name in product_names}
    data_path = Path(data_dir)

    if data_path.exists():
        print(f"ğŸ” æ‰«æ {data_dir} æ–­ç‚¹...")
        for json_file in data_path.glob('*_products_*.json'):
            if json_file.name.startswith('all_products'): continue
            
            # æ–‡ä»¶ååŒ¹é…
            match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
            if not match: continue
            
            # æ³¨æ„ï¼šæ–‡ä»¶åæ˜¯ safe_nameï¼Œéœ€è¦ç®€å•åŒ¹é…å›åŸå (æ­¤å¤„ç®€åŒ–å¤„ç†)
            # å®é™…é¡¹ç›®ä¸­å»ºè®®åœ¨æ–‡ä»¶åä¸­ä¿ç•™æ›´ç²¾ç¡®çš„ ID æˆ–å“ˆå¸Œï¼Œæˆ–è€…åœ¨è¿™é‡Œåšæ¨¡ç³ŠåŒ¹é…
            p_safe_name = match.group(1)
            
            # åå‘æŸ¥æ‰¾å¯¹åº”çš„åŸå§‹ task name
            target_task = None
            for name in product_names:
                if re.sub(r'[<>:"/\\|?*]', "_", name)[:50] == p_safe_name:
                    target_task = name
                    break
            
            if target_task and target_task in tasks_progress:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if data and isinstance(data, list):
                        last_item = data[-1]
                        # ä¼˜å…ˆå– pageï¼Œæ²¡æœ‰åˆ™å– index
                        current = int(last_item.get('page', 0))
                        if not current:
                            current = int(last_item.get('index', len(data)))
                        
                        if current > tasks_progress[target_task]:
                            tasks_progress[target_task] = current
                except: pass

    final_tasks = []
    for name, progress in tasks_progress.items():
        # eBay çš„ max_count æ˜¯æ¡æ•°ï¼Œprogress æ˜¯é¡µæ•°ã€‚
        # è¿™é‡Œåªè¦ progress > 0 å°±æ‰“å°æ¢å¤ä¿¡æ¯ï¼Œå…·ä½“æ˜¯å¦çˆ¬å®Œç”± crawl å†…éƒ¨ current_count åˆ¤æ–­
        if progress > 0:
            print(f"  ğŸ”„ æ¢å¤ä»»åŠ¡: {name} (ä» Page {progress} ç»§ç»­)")
        final_tasks.append((name, progress))

    return sorted(final_tasks, key=lambda x: x[0])

# ==================== ä¸»å…¥å£ ====================
if __name__ == "__main__":
    print("eBay çˆ¬è™« (æ ‡å‡†ç‰ˆ)")
    print("="*60)

    WORKER_COUNT = 2      # eBay å»ºè®®ä½å¹¶å‘
    BASE_PORT = 9333      # ç‹¬ç«‹ç«¯å£æ®µ
    MAX_CRAWL = 100       # ç›®æ ‡æŠ“å–æ¡æ•°
    OUTPUT_DIR = 'ebay_data'
    TASK_FILE = 'clothing_leaf_names.json'

    all_tasks = get_tasks_from_file(TASK_FILE, MAX_CRAWL, OUTPUT_DIR)
    
    if all_tasks:
        print(f"ğŸ“¦ ä»»åŠ¡æ•°: {len(all_tasks)}")
        manager = MultiCrawlerManager(
            crawler_class=EbayCrawler, 
            base_port=BASE_PORT, 
            workers=WORKER_COUNT
        )
        try:
            asyncio.run(manager.run(all_tasks, MAX_CRAWL, OUTPUT_DIR))
        except KeyboardInterrupt:
            print("ğŸ›‘ åœæ­¢")
    else:
        print("ğŸ‰ æ— ä»»åŠ¡")
