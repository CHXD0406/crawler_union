"""
å°ç±³æœ‰å“ xiaomiyoupin.com å•†å“çˆ¬è™«
å°è£…æˆå‡½æ•°ï¼Œè¾“å…¥å•†å“åç§°å’Œé¡µæ•°ï¼Œçˆ¬å–å¯¹åº”å•†å“å¯¹åº”é¡µæ•°çš„ä¿¡æ¯
æ³¨æ„ï¼šå°ç±³æœ‰å“æ— éœ€ç™»å½•å³å¯çˆ¬å–æ•°æ®
"""
import asyncio
import json
import os
import re
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup


class XiaomiYoupinCrawler:
    """å°ç±³æœ‰å“çˆ¬è™«ç±»"""
    
    def __init__(self, headless=True, save_html=False):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å‚æ•°:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            save_html: æ˜¯å¦ä¿å­˜HTMLæ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼‰
        """
        self.headless = headless
        self.save_html = save_html
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        
    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆä½¿ç”¨ Edgeï¼‰"""
        if not self.playwright:
            self.playwright = await async_playwright().start()
        
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            channel='msedge',
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        )
        
        self.page = await self.context.new_page()
        
        # éšè—webdriverç‰¹å¾
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    async def check_verification(self):
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦éœ€è¦éªŒè¯
        
        è¿”å›:
            bool: Trueè¡¨ç¤ºéœ€è¦éªŒè¯ï¼ŒFalseè¡¨ç¤ºä¸éœ€è¦
        """
        try:
            current_url = self.page.url
            
            verification_keywords = ['verify', 'captcha', 'challenge', 'security', 'validate', 
                                    'éªŒè¯', 'å®‰å…¨éªŒè¯', 'äººæœºéªŒè¯', 'æ»‘å—éªŒè¯']
            if any(keyword in current_url.lower() for keyword in verification_keywords):
                return True
            
            try:
                title = await self.page.title()
                if any(keyword in title.lower() for keyword in verification_keywords):
                    return True
            except:
                pass
            
            verification_selectors = [
                'iframe[src*="captcha"]',
                'iframe[src*="verify"]',
                '.captcha',
                '.verify',
                '#captcha',
                '#verify',
                '[class*="captcha"]',
                '[class*="verify"]',
                '[class*="slider"]',
            ]
            
            for selector in verification_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        is_visible = await element.is_visible()
                        if is_visible:
                            return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹éªŒè¯æ—¶å‡ºé”™: {e}")
            return False
    
    async def wait_for_verification(self):
        """
        ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯
        """
        print("\n" + "="*60)
        print("ğŸ”’ æ£€æµ‹åˆ°éœ€è¦éªŒè¯")
        print("="*60)
        print("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯æ“ä½œï¼š")
        print("  1. å®Œæˆæ»‘å—éªŒè¯ã€å›¾ç‰‡éªŒè¯ç­‰")
        print("  2. ç¡®ä¿éªŒè¯é€šè¿‡å")
        print("-"*60)
        print(">>> å®Œæˆåè¯·æŒ‰ Enter é”®ç»§ç»­... <<<")
        print("="*60)
        
        await asyncio.get_event_loop().run_in_executor(None, input)
        
        print("\næ­£åœ¨æ£€æŸ¥éªŒè¯çŠ¶æ€...")
        await asyncio.sleep(1)
        
        still_need_verification = await self.check_verification()
        if not still_need_verification:
            print("âœ“ éªŒè¯å·²å®Œæˆï¼")
            return True
        else:
            print("âš ï¸ ä»æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œè¯·å†æ¬¡å°è¯•...")
            return False
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.page:
            try:
                await self.page.close()
            except:
                pass
        if self.context:
            try:
                await self.context.close()
            except:
                pass
        if self.browser:
            try:
                await self.browser.close()
            except:
                pass
        if hasattr(self, 'playwright') and self.playwright:
            try:
                await self.playwright.stop()
            except:
                pass
    
    async def scroll_to_load(self, scroll_times=5):
        """
        æ»šåŠ¨é¡µé¢ä»¥åŠ è½½åŠ¨æ€å†…å®¹
        
        å‚æ•°:
            scroll_times: æ»šåŠ¨æ¬¡æ•°
        """
        for i in range(scroll_times):
            await self.page.evaluate('window.scrollBy(0, window.innerHeight)')
            await asyncio.sleep(0.5)
        # æ»šå›é¡¶éƒ¨
        await self.page.evaluate('window.scrollTo(0, 0)')
        await asyncio.sleep(0.3)
    
    async def get_total_pages(self):
        """
        æ£€æµ‹å½“å‰æœç´¢ç»“æœçš„æ€»é¡µæ•°
        
        è¿”å›:
            int: æ€»é¡µæ•°ï¼Œå¦‚æœæ— æ³•æ£€æµ‹åˆ™è¿”å› 1
        """
        try:
            # æ»šåŠ¨åˆ°åº•éƒ¨ä»¥ç¡®ä¿åˆ†é¡µç»„ä»¶åŠ è½½
            await self.page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await asyncio.sleep(1)
            
            # æ–¹å¼1: æŸ¥æ‰¾åˆ†é¡µç»„ä»¶ä¸­çš„æ€»é¡µæ•°
            # å¸¸è§çš„åˆ†é¡µé€‰æ‹©å™¨
            pagination_selectors = [
                '.pagination',
                '.pager',
                '[class*="pagination"]',
                '[class*="pager"]',
                '[class*="page-list"]',
            ]
            
            for selector in pagination_selectors:
                try:
                    pagination = await self.page.query_selector(selector)
                    if pagination:
                        # è·å–åˆ†é¡µåŒºåŸŸçš„æ‰€æœ‰é¡µç 
                        page_items = await pagination.query_selector_all('a, button, li, span')
                        max_page = 1
                        for item in page_items:
                            text = await item.inner_text()
                            text = text.strip()
                            # å°è¯•æå–æ•°å­—
                            if text.isdigit():
                                page_num = int(text)
                                if page_num > max_page:
                                    max_page = page_num
                        if max_page > 1:
                            return max_page
                except:
                    continue
            
            # æ–¹å¼2: æŸ¥æ‰¾"ä¸‹ä¸€é¡µ"æŒ‰é’®æ˜¯å¦ç¦ç”¨æˆ–ä¸å­˜åœ¨
            next_page_selectors = [
                '.pagination-next',
                '.page-next',
                '[class*="next"]',
                'a:has-text("ä¸‹ä¸€é¡µ")',
                'button:has-text("ä¸‹ä¸€é¡µ")',
            ]
            
            has_next = False
            for selector in next_page_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        is_visible = await elem.is_visible()
                        is_disabled = await elem.get_attribute('disabled')
                        class_attr = await elem.get_attribute('class') or ''
                        
                        # æ£€æŸ¥æ˜¯å¦è¢«ç¦ç”¨
                        if is_visible and not is_disabled and 'disabled' not in class_attr:
                            has_next = True
                            break
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µæŒ‰é’®æˆ–è¢«ç¦ç”¨ï¼Œè¯´æ˜åªæœ‰1é¡µ
            if not has_next:
                return 1
            
            # æ–¹å¼3: æŸ¥æ‰¾é¡µé¢ä¸­æ˜¯å¦æœ‰"å…± X é¡µ"æˆ–ç±»ä¼¼æ–‡æœ¬
            try:
                page_text = await self.page.inner_text('body')
                # åŒ¹é… "å…± X é¡µ" æˆ– "å…±Xé¡µ" æˆ– "æ€»å…± X é¡µ"
                match = re.search(r'å…±\s*(\d+)\s*é¡µ|æ€»å…±\s*(\d+)\s*é¡µ', page_text)
                if match:
                    total = match.group(1) or match.group(2)
                    return int(total)
            except:
                pass
            
            # é»˜è®¤è¿”å›ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œè®©ç¨‹åºç»§ç»­å°è¯•ç¿»é¡µ
            return 999
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹æ€»é¡µæ•°æ—¶å‡ºé”™: {e}")
            return 999
    
    async def check_has_next_page(self):
        """
        æ£€æµ‹æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
        
        è¿”å›:
            bool: True è¡¨ç¤ºæœ‰ä¸‹ä¸€é¡µï¼ŒFalse è¡¨ç¤ºæ²¡æœ‰
        """
        try:
            # æ£€æŸ¥ä¸‹ä¸€é¡µæŒ‰é’®æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨
            next_page_selectors = [
                '.pagination-next:not(.disabled)',
                '.page-next:not(.disabled)',
                '[class*="next"]:not([class*="disabled"])',
                'a:has-text("ä¸‹ä¸€é¡µ"):not(.disabled)',
                'button:has-text("ä¸‹ä¸€é¡µ"):not(:disabled)',
            ]
            
            for selector in next_page_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        is_visible = await elem.is_visible()
                        if is_visible:
                            # æ£€æŸ¥æ˜¯å¦è¢«ç¦ç”¨
                            is_disabled = await elem.get_attribute('disabled')
                            class_attr = await elem.get_attribute('class') or ''
                            aria_disabled = await elem.get_attribute('aria-disabled')
                            
                            if not is_disabled and 'disabled' not in class_attr.lower() and aria_disabled != 'true':
                                return True
                except:
                    continue
            
            # æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ä¸‹ä¸€é¡µæŒ‰é’®
            return False
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹ä¸‹ä¸€é¡µæ—¶å‡ºé”™: {e}")
            return False
    
    def extract_products(self, html_content, page_num):
        """
        ä»HTMLä¸­æå–å•†å“ä¿¡æ¯ï¼ˆé’ˆå¯¹å°ç±³æœ‰å“é¡µé¢ç»“æ„ï¼‰
        
        å°ç±³æœ‰å“å•†å“åˆ—è¡¨ç»“æ„å¯èƒ½ä¸ºï¼š
        - å•†å“å®¹å™¨å¸¦æœ‰ data-gid æˆ– data-pid å±æ€§
        - æˆ–è€…åŒ…å« goods-item / product-item ç±»å
        
        å‚æ•°:
            html_content: HTMLå†…å®¹
            page_num: é¡µç 
            
        è¿”å›:
            products: å•†å“åˆ—è¡¨
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å•†å“å®¹å™¨ - å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
        product_containers = []
        
        # æ–¹å¼1: å¸¦æœ‰ data-gid å±æ€§çš„å…ƒç´ 
        containers = soup.find_all(attrs={'data-gid': True})
        if containers:
            product_containers = containers
        
        # æ–¹å¼2: å¸¦æœ‰ data-pid å±æ€§çš„å…ƒç´ 
        if not product_containers:
            containers = soup.find_all(attrs={'data-pid': True})
            if containers:
                product_containers = containers
        
        # æ–¹å¼3: åŒ…å« goods-item ç±»çš„å…ƒç´ 
        if not product_containers:
            containers = soup.find_all(class_=re.compile(r'goods[-_]?item|product[-_]?item|search[-_]?item', re.I))
            if containers:
                product_containers = containers
        
        # æ–¹å¼4: åŒ…å«å•†å“é“¾æ¥çš„ a æ ‡ç­¾å®¹å™¨
        if not product_containers:
            links = soup.find_all('a', href=re.compile(r'/detail|/product|/goods|gid=', re.I))
            for link in links:
                parent = link.find_parent(['div', 'li', 'article'])
                if parent and parent not in product_containers:
                    product_containers.append(parent)
        
        # æ–¹å¼5: æŸ¥æ‰¾åŒ…å«ä»·æ ¼çš„å•†å“å—
        if not product_containers:
            price_elements = soup.find_all(class_=re.compile(r'price', re.I))
            for price_elem in price_elements:
                parent = price_elem.find_parent(['div', 'li', 'article'], class_=True)
                if parent and parent not in product_containers:
                    # ç¡®ä¿æ˜¯å•†å“å®¹å™¨è€Œä¸æ˜¯å…¶ä»–å…ƒç´ 
                    if parent.find('img') and parent.find('a'):
                        product_containers.append(parent)
        
        print(f"æ‰¾åˆ° {len(product_containers)} ä¸ªå•†å“å®¹å™¨")
        
        for idx, container in enumerate(product_containers, 1):
            try:
                product = {
                    'page': page_num,
                    'index': idx
                }
                
                # 1. æå–å•†å“ID
                product_id = (container.get('data-gid', '') or 
                             container.get('data-pid', '') or 
                             container.get('data-id', ''))
                product['product_id'] = product_id
                
                # 2. æå–å•†å“é“¾æ¥
                link_elem = container.find('a', href=True)
                href = ''
                if link_elem:
                    href = link_elem.get('href', '')
                    if href:
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://www.xiaomiyoupin.com' + href
                        href = href.replace('&amp;', '&')
                    
                    # ä»é“¾æ¥ä¸­æå–å•†å“ID
                    if not product_id:
                        id_match = re.search(r'gid=(\d+)|/detail/(\d+)|/product/(\d+)', href)
                        if id_match:
                            product_id = id_match.group(1) or id_match.group(2) or id_match.group(3)
                            product['product_id'] = product_id
                
                product['link'] = href
                
                # 3. æå–å•†å“å›¾ç‰‡å’Œåç§°
                img_elem = container.find('img')
                product_image = ''
                title_from_img = ''
                if img_elem:
                    img_src = (img_elem.get('src', '') or 
                              img_elem.get('data-src', '') or 
                              img_elem.get('data-lazy-src', '') or
                              img_elem.get('data-original', ''))
                    if img_src:
                        if img_src.startswith('//'):
                            img_src = 'https:' + img_src
                        elif img_src.startswith('/'):
                            img_src = 'https://www.xiaomiyoupin.com' + img_src
                        # è¿‡æ»¤å ä½å›¾
                        if 'placeholder' not in img_src.lower() and 'loading' not in img_src.lower():
                            product_image = img_src
                    title_from_img = img_elem.get('alt', '')
                
                product['image'] = product_image
                
                # 4. æå–å•†å“åç§°
                title = ''
                # å°è¯•å¤šç§é€‰æ‹©å™¨
                title_selectors = [
                    ('[class*="name"]', None),
                    ('[class*="title"]', None),
                    ('h3', None),
                    ('h4', None),
                    ('.goods-name', None),
                    ('.product-name', None),
                    ('.item-name', None),
                ]
                
                for selector, _ in title_selectors:
                    title_elem = container.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title and len(title) > 2:
                            break
                
                if not title:
                    title = title_from_img
                
                title = ' '.join(title.split()) if title else ''
                product['title'] = title
                
                # 5. æå–å”®ä»·
                price = ''
                price_selectors = [
                    '[class*="price"]',
                    '[class*="sale"]',
                    '.goods-price',
                    '.product-price',
                    '.item-price',
                ]
                
                for selector in price_selectors:
                    price_elems = container.select(selector)
                    for price_elem in price_elems:
                        price_text = price_elem.get_text(strip=True)
                        # æå–æ•°å­—ä»·æ ¼
                        price_match = re.search(r'[\d.]+', price_text)
                        if price_match:
                            price = price_match.group()
                            break
                    if price:
                        break
                
                product['price'] = price
                
                # 6. æå–åŸä»·ï¼ˆå¦‚æœæœ‰ï¼‰
                original_price = ''
                orig_price_selectors = [
                    '[class*="origin"]',
                    '[class*="market"]',
                    '[class*="old"]',
                    'del',
                    's',
                ]
                
                for selector in orig_price_selectors:
                    orig_elem = container.select_one(selector)
                    if orig_elem:
                        orig_text = orig_elem.get_text(strip=True)
                        orig_match = re.search(r'[\d.]+', orig_text)
                        if orig_match:
                            original_price = orig_match.group()
                            break
                
                product['original_price'] = original_price
                
                # 7. æå–æŠ˜æ‰£ä¿¡æ¯
                discount = ''
                discount_selectors = [
                    '[class*="discount"]',
                    '[class*="off"]',
                    '[class*="tag"]',
                ]
                
                for selector in discount_selectors:
                    discount_elem = container.select_one(selector)
                    if discount_elem:
                        discount_text = discount_elem.get_text(strip=True)
                        if 'æŠ˜' in discount_text or '%' in discount_text or 'off' in discount_text.lower():
                            discount = discount_text
                            break
                
                product['discount'] = discount
                
                # 8. æå–è¯„ä»·æ•°/é”€é‡
                sales = ''
                sales_selectors = [
                    '[class*="comment"]',
                    '[class*="review"]',
                    '[class*="sale"]',
                    '[class*="sold"]',
                ]
                
                for selector in sales_selectors:
                    sales_elem = container.select_one(selector)
                    if sales_elem:
                        sales_text = sales_elem.get_text(strip=True)
                        if re.search(r'\d+', sales_text):
                            sales = sales_text
                            break
                
                product['sales'] = sales
                
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå•†å“ï¼ˆå¿…é¡»æœ‰æ ‡é¢˜å’Œé“¾æ¥ï¼‰
                is_valid = bool(product.get('title') and product.get('link'))
                
                if is_valid:
                    products.append(product)
                    title_preview = product['title'][:40] + '...' if len(product['title']) > 40 else product['title']
                    price_display = f"Â¥{product['price']}" if product.get('price') else 'N/A'
                    print(f"å•†å“ {len(products)}: {title_preview} - {price_display}")
                
            except Exception as e:
                print(f"æå–å•†å“ {idx} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\næ€»å…±æå–åˆ° {len(products)} ä¸ªæœ‰æ•ˆå•†å“")
        return products


async def crawl_products_automated(products, num_pages_per_product, headless=False, save_html=False, output_dir='xiaomiyoupin_data'):
    """
    æŒ‰ç…§è‡ªåŠ¨åŒ–æµç¨‹çˆ¬å–å¤šä¸ªå•†å“çš„å¤šé¡µæ•°æ®
    
    å‚æ•°:
        products: å•†å“åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ['æ‰‹æœº', 'è€³æœº', 'ç”µè„‘']
        num_pages_per_product: æ¯ä¸ªå•†å“è¦çˆ¬å–çš„é¡µæ•°
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰
        save_html: æ˜¯å¦ä¿å­˜HTMLæ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›:
        all_products: æ‰€æœ‰å•†å“åˆ—è¡¨
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    crawler = XiaomiYoupinCrawler(headless=headless, save_html=save_html)
    all_products = []
    
    try:
        await crawler.init_browser()
        
        # æ‰“å¼€é¦–é¡µ
        url = "https://www.xiaomiyoupin.com"
        print(f"\n{'='*60}")
        print(f"æ‰“å¼€ç½‘é¡µ: {url}")
        print(f"{'='*60}")
        await crawler.page.goto(url, wait_until='domcontentloaded', timeout=60000)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(3)
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦éªŒè¯
        while await crawler.check_verification():
            success = await crawler.wait_for_verification()
            if success:
                break
        
        # éå†æ¯ä¸ªå•†å“
        for product_idx, product_name in enumerate(products, 1):
            print(f"\n{'='*60}")
            print(f"å•†å“ {product_idx}/{len(products)}: {product_name}")
            print(f"{'='*60}")
            
            product_products = []
            
            # ä½¿ç”¨URLç›´æ¥æœç´¢
            # å°ç±³æœ‰å“æœç´¢URLæ ¼å¼
            search_url = f"https://www.xiaomiyoupin.com/search?keyword={product_name}"
            print(f"\næ‰“å¼€æœç´¢é¡µé¢: {search_url}")
            
            try:
                await crawler.page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(3)
                
                # æ£€æµ‹æ˜¯å¦éœ€è¦éªŒè¯
                while await crawler.check_verification():
                    success = await crawler.wait_for_verification()
                    if success:
                        break
                
            except Exception as e:
                print(f"âš ï¸ æ‰“å¼€æœç´¢é¡µé¢å¤±è´¥: {e}")
                continue
            
            # æ£€æµ‹æ€»é¡µæ•°
            total_pages = await crawler.get_total_pages()
            actual_pages = min(num_pages_per_product, total_pages)
            
            if total_pages < num_pages_per_product:
                print(f"\nğŸ“„ æ£€æµ‹åˆ°è¯¥å•†å“åªæœ‰ {total_pages} é¡µæœç´¢ç»“æœï¼ˆè®¾å®šçˆ¬å– {num_pages_per_product} é¡µï¼‰")
                print(f"   å°†çˆ¬å–æ‰€æœ‰ {total_pages} é¡µåç»§ç»­ä¸‹ä¸€ä¸ªå•†å“")
            else:
                print(f"\nğŸ“„ æ£€æµ‹åˆ°è¯¥å•†å“æœ‰ {total_pages}+ é¡µï¼Œå°†çˆ¬å–å‰ {num_pages_per_product} é¡µ")
            
            # éå†æ¯ä¸ªé¡µé¢
            for page_num in range(1, actual_pages + 1):
                print(f"\n  {'-'*50}")
                print(f"  ç¬¬ {page_num}/{num_pages_per_product} é¡µ")
                print(f"  {'-'*50}")
                
                try:
                    # æ£€æµ‹éªŒè¯
                    while await crawler.check_verification():
                        success = await crawler.wait_for_verification()
                        if success:
                            break
                    
                    # ç­‰å¾…é¡µé¢ç¨³å®š
                    await asyncio.sleep(2)
                    try:
                        await crawler.page.wait_for_load_state('domcontentloaded', timeout=15000)
                    except:
                        pass
                    
                    # æ»šåŠ¨åŠ è½½åŠ¨æ€å†…å®¹
                    print("  æ»šåŠ¨é¡µé¢åŠ è½½å•†å“...")
                    await crawler.scroll_to_load(scroll_times=5)
                    
                    # è·å–HTMLå†…å®¹
                    html_content = await crawler.page.content()
                    
                    if html_content:
                        # ä¿å­˜HTML
                        if save_html:
                            html_file = os.path.join(output_dir, f"{product_name}_page_{page_num}.html")
                            with open(html_file, 'w', encoding='utf-8') as f:
                                f.write(html_content)
                            print(f"  âœ“ HTMLå·²ä¿å­˜: {html_file}")
                        
                        # æå–å•†å“ä¿¡æ¯
                        products_data = crawler.extract_products(html_content, page_num)
                        product_products.extend(products_data)
                        print(f"  âœ“ ç¬¬ {page_num} é¡µå®Œæˆï¼Œæå–åˆ° {len(products_data)} ä¸ªå•†å“")
                    else:
                        print(f"  âš ï¸ ç¬¬ {page_num} é¡µæ— æ³•è·å–HTMLå†…å®¹")
                
                except Exception as e:
                    print(f"  âš ï¸ ç¬¬ {page_num} é¡µçˆ¬å–å‡ºé”™: {e}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œå°è¯•ç¿»é¡µ
                if page_num < actual_pages:
                    # å…ˆæ£€æµ‹æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                    has_next = await crawler.check_has_next_page()
                    if not has_next:
                        print(f"  â„¹ï¸ æ²¡æœ‰æ›´å¤šé¡µé¢äº†ï¼Œå½“å‰å•†å“çˆ¬å–å®Œæˆï¼ˆå…± {page_num} é¡µï¼‰")
                        break
                    
                    print(f"  ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                    try:
                        # å°è¯•ä½¿ç”¨é€‰æ‹©å™¨ç‚¹å‡»ä¸‹ä¸€é¡µ
                        next_page_selectors = [
                            '.pagination-next',
                            '.page-next',
                            '[class*="next"]',
                            'a:has-text("ä¸‹ä¸€é¡µ")',
                            'button:has-text("ä¸‹ä¸€é¡µ")',
                        ]
                        
                        clicked = False
                        for selector in next_page_selectors:
                            try:
                                elem = await crawler.page.query_selector(selector)
                                if elem:
                                    is_visible = await elem.is_visible()
                                    if is_visible:
                                        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨
                                        is_disabled = await elem.get_attribute('disabled')
                                        class_attr = await elem.get_attribute('class') or ''
                                        if not is_disabled and 'disabled' not in class_attr.lower():
                                            await elem.click()
                                            clicked = True
                                            print("  âœ“ å·²ç‚¹å‡»ä¸‹ä¸€é¡µ")
                                            break
                            except:
                                continue
                        
                        if not clicked:
                            # å°è¯•é€šè¿‡URLç¿»é¡µ
                            current_url = crawler.page.url
                            if 'page=' in current_url:
                                new_url = re.sub(r'page=\d+', f'page={page_num + 1}', current_url)
                            else:
                                separator = '&' if '?' in current_url else '?'
                                new_url = f"{current_url}{separator}page={page_num + 1}"
                            
                            await crawler.page.goto(new_url, wait_until='domcontentloaded', timeout=60000)
                            print(f"  âœ“ é€šè¿‡URLè·³è½¬åˆ°ç¬¬ {page_num + 1} é¡µ")
                        
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        print(f"  âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
                        break
            
            # ä¿å­˜å½“å‰å•†å“çš„æ•°æ®
            if product_products:
                all_products.extend(product_products)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                # ä¿å­˜JSON
                json_file = os.path.join(output_dir, f"{product_name}_products_{timestamp}.json")
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(product_products, f, ensure_ascii=False, indent=2)
                print(f"\nâœ“ {product_name} çš„JSONæ•°æ®å·²ä¿å­˜: {json_file}")
                
                print(f"âœ“ {product_name} å®Œæˆï¼Œå…±æå– {len(product_products)} ä¸ªå•†å“")
            else:
                print(f"\nâš ï¸ {product_name} æœªæå–åˆ°ä»»ä½•å•†å“")
        
        # ä¿å­˜æ‰€æœ‰å•†å“çš„æ€»æ•°æ®
        if all_products:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            all_json_file = os.path.join(output_dir, f"all_products_{timestamp}.json")
            with open(all_json_file, 'w', encoding='utf-8') as f:
                json.dump(all_products, f, ensure_ascii=False, indent=2)
            print(f"\n{'='*60}")
            print(f"âœ“ æ‰€æœ‰å•†å“æ•°æ®å·²ä¿å­˜: {all_json_file}")
            print(f"æ€»å…±çˆ¬å–åˆ° {len(all_products)} ä¸ªå•†å“")
            print(f"{'='*60}")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await crawler.close()
    
    return all_products


def get_crawled_products(data_dir='xiaomiyoupin_data', check_html=True):
    """
    ä»æ•°æ®ç›®å½•ä¸­æå–å·²çˆ¬å–çš„å•†å“åç§°
    
    å‚æ•°:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        check_html: æ˜¯å¦ä¹Ÿæ£€æŸ¥ HTML æ–‡ä»¶
    
    è¿”å›:
        set: å·²çˆ¬å–çš„å•†å“åç§°é›†åˆ
    """
    crawled_products = set()
    data_path = Path(data_dir)
    
    if not data_path.exists():
        return crawled_products
    
    # æŸ¥æ‰¾æ‰€æœ‰ *_products_*.json æ–‡ä»¶
    for json_file in data_path.glob('*_products_*.json'):
        if json_file.name.startswith('all_products'):
            continue
        match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
        if match:
            product_name = match.group(1)
            crawled_products.add(product_name)
    
    if check_html:
        for html_file in data_path.glob('*_page_*.html'):
            match = re.match(r'^(.+?)_page_\d+\.html$', html_file.name)
            if match:
                product_name = match.group(1)
                crawled_products.add(product_name)
    
    return crawled_products


def filter_products(products_list, crawled_products):
    """
    ä»å•†å“åˆ—è¡¨ä¸­ç§»é™¤å·²çˆ¬å–çš„å•†å“
    
    å‚æ•°:
        products_list: åŸå§‹å•†å“åˆ—è¡¨
        crawled_products: å·²çˆ¬å–çš„å•†å“é›†åˆ
    
    è¿”å›:
        tuple: (æœªçˆ¬å–çš„å•†å“åˆ—è¡¨, å·²çˆ¬å–çš„å•†å“åˆ—è¡¨)
    """
    uncrawled = []
    crawled = []
    
    for product in products_list:
        if product in crawled_products:
            crawled.append(product)
        else:
            uncrawled.append(product)
    
    return uncrawled, crawled


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("å°ç±³æœ‰å“ xiaomiyoupin.com å•†å“çˆ¬è™«")
    print("="*60)
    print("æ³¨æ„ï¼šå°ç±³æœ‰å“æ— éœ€ç™»å½•å³å¯çˆ¬å–æ•°æ®")
    print("="*60)
    
    # å•†å“åˆ—è¡¨ - å°ç±³æœ‰å“ç‰¹è‰²å•†å“
    products_list = [
    'Aå­—è£™', 'POLOè¡«', 'Tæ¤', 'chokeré¡¹åœˆ', 'ä¸å·¾', 'ä¹ç¦é‹', 'äºšå…‹åŠ›é¦–é¥°', 'äººå­—æ‹–', 
    'ä¼‘é—²è£¤', 'ä¿æš–è¢œ', 'å…‰å­¦çœ¼é•œ', 'å†…è£¤', 'å‡‰é‹', 'åˆ‡å°”è¥¿é´', 'åŒ–å¦†åˆ·', 'åŒ–å¦†åŒ…', 
    'åŠèº«è£™', 'å•è‚©åŒ…', 'å¡å…¶è£¤', 'å«è¡£', 'åŒè‚©åŒ…', 'å‘å¤¹', 'å‘å¸¦', 'å‘ç®', 'å‘é¥°', 
    'å£ç½©', 'å£è¢‹å·¾', 'å¤é¾™æ°´', 'åˆé‡‘é¦–é¥°', 'åŠå¸¦', 'åŠå¸¦è£™', 'å–‡å­è£¤', 'å›´å·¾', 
    'å¤§è¡£', 'å¤ªé˜³å¸½', 'å¤ªé˜³é•œ', 'å¤¹å…‹', 'å®çŸ³', 'å®¶å±…æœ', 'å®½æªå¸½', 'å°é»‘è£™', 
    'å·¥è£…è£¤', 'å¸†å¸ƒè¢‹', 'å¸†å¸ƒé‹', 'å¸½å­', 'å¹³åº•é‹', 'å¾·æ¯”é‹', 'æ€€è¡¨', 'æˆ’æŒ‡', 
    'æ‰‹å¥—', 'æ‰‹æ‹¿åŒ…', 'æ‰‹æåŒ…', 'æ‰‹æœºå£³', 'æ‰‹è¡¨', 'æ‰‹é“¾', 'æ‰˜ç‰¹åŒ…', 'æŠ¤ç…§å¤¹', 
    'æŠ«è‚©', 'æ‹–é‹', 'æ–‡åˆ›åŒ…è¢‹', 'æ–‡èƒ¸', 'æ–œæŒåŒ…', 'æ—…è¡Œæ”¶çº³åŒ…', 'æ™šå®´åŒ…', 'æ™šç¤¼æœ', 
    'æ™ºèƒ½æˆ’æŒ‡', 'æ™ºèƒ½æ‰‹ç¯', 'æ™ºèƒ½æ‰‹è¡¨', 'æœºæ¢°è¡¨', 'æ¡çº¹è¡«', 'æ¿é‹', 'æ£’çƒå¸½', 
    'æ¯›å‘¢å¤–å¥—', 'æ¯›çº¿å¸½', 'æ¯›è¡£', 'æ°´æ¡¶åŒ…', 'æ²™æ»©å·¾', 'æ³¢å£«é¡¿åŒ…', 'æ³³è¡£', 'æ·¡é¦™æ°´', 
    'æ·¡é¦™ç²¾', 'æ¸”å¤«å¸½', 'ç‡•å°¾æœ', 'ç‰›ä»”å¤–å¥—', 'ç‰›ä»”å¤¹å…‹', 'ç‰›ä»”è£¤', 'ç‰›æ´¥é‹', 
    'ç›ä¸½çé‹', 'ç¯ä¿åŒ…è¢‹', 'çç é¡¹é“¾', 'çç é¦–é¥°', 'ç‘œä¼½è£¤', 'ç”·å£«è¥¿è£…', 'ç™½è¡¬è¡«', 
    'ç™¾è¤¶è£™', 'çš®å¸¦', 'çš®è¡£', 'çœ¼é•œæ¡†', 'ç¡è¢', 'çŸ­è£¤', 'çŸ³è‹±è¡¨', 'ç§‘æŠ€è®¾å¤‡', 
    'ç©†å‹’é‹', 'ç´§èº«è£¤', 'ç¼–ç»‡é¥°å“', 'ç½©è¡«', 'ç¾å¦†è›‹', 'ç¾½ç»’æœ', 'è€³æœºä¿æŠ¤å¥—', 
    'è€³ç¯', 'è€³çº¿', 'è€³ç½©', 'è€³é’‰', 'èƒŒå¿ƒ', 'èƒ¸è¡£', 'èƒ¸é’ˆ', 'è„šé“¾', 'è…°åŒ…', 
    'è…°å¸¦', 'èŠ­è•¾é‹', 'èŒ¶æ­‡è£™', 'è‰å¸½', 'è¡¬è¡«', 'è¡¬è¡«è£™', 'è¢–æ‰£', 'è¥¿è£…å¤–å¥—', 
    'è¥¿è£…å¥—è£…', 'è¥¿è£¤', 'è´é›·å¸½', 'è·‘æ­¥é‹', 'è¸é´', 'è¿åŠ¨å†…è¡£', 'è¿åŠ¨å¤´å¸¦', 
    'è¿åŠ¨æ‰‹å¥—', 'è¿åŠ¨æ°´å£¶', 'è¿åŠ¨è¡«', 'è¿åŠ¨é‹', 'è¿ä½“è£¤', 'è¿è¡£è£™', 'é‡‘é“¶é¦–é¥°', 
    'é’ˆç»‡è¡«', 'é’»çŸ³', 'é“…ç¬”è£™', 'é“¾æ¡åŒ…', 'é˜”è…¿è£¤', 'é›¨é‹', 'é›ªåœ°é´', 'é¡¹é“¾', 
    'é¢†å¸¦', 'é¢†ç»“', 'é¢ˆæ•', 'é£è¡£', 'é£è¡Œå‘˜å¤¹å…‹', 'é¦™ä½“å–·é›¾', 'é¦™æ°´', 'é©¬ä¸é´', 
    'é©¬ç”²', 'é«˜è·Ÿé‹', 'é»‘è‰²ç´§èº«è£¤'
]
    num_pages = 20  # æ¯ä¸ªå•†å“çˆ¬å–çš„é¡µæ•°
    
    # è‡ªåŠ¨æ£€æŸ¥å¹¶è¿‡æ»¤å·²çˆ¬å–çš„å•†å“
    crawled_products = get_crawled_products('xiaomiyoupin_data', check_html=True)
    print(f"\nå·²çˆ¬å–çš„å•†å“ ({len(crawled_products)} ä¸ª):")
    for product in sorted(crawled_products):
        print(f"  - {product}")
    
    products_list, _ = filter_products(products_list, crawled_products)
    
    print(f"\nè¿‡æ»¤åå¾…çˆ¬å–çš„å•†å“ ({len(products_list)} ä¸ª):")
    for product in sorted(products_list):
        print(f"  - {product}")
    
    if not products_list:
        print("\næ‰€æœ‰å•†å“å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€å†æ¬¡è¿è¡Œã€‚")
    else:
        print("\nå¼€å§‹çˆ¬å–å•†å“...")
        all_products = asyncio.run(crawl_products_automated(
            products=products_list,
            num_pages_per_product=num_pages,
            headless=False,
            save_html=True,
            output_dir='xiaomiyoupin_data'
        ))
        
        print(f"\nçˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªå•†å“")
