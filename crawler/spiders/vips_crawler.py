"""
å”¯å“ä¼š VIP.com å•†å“çˆ¬è™«
å°è£…æˆå‡½æ•°ï¼Œè¾“å…¥å•†å“åç§°å’Œé¡µæ•°ï¼Œçˆ¬å–å¯¹åº”å•†å“å¯¹åº”é¡µæ•°çš„ä¿¡æ¯
"""
import asyncio
import json
import os
import time
import random
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup
import re
import pyautogui
import pyperclip


# Cookies æ–‡ä»¶è·¯å¾„
COOKIES_FILE = Path(__file__).parent / 'vips_cookies.json'


class VipsCrawler:
    """å”¯å“ä¼š VIP.com çˆ¬è™«ç±»"""
    
    def __init__(self, headless=True, save_html=False, cookies_file=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å‚æ•°:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            save_html: æ˜¯å¦ä¿å­˜HTMLæ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼‰
            cookies_file: cookiesæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        """
        self.headless = headless
        self.save_html = save_html
        self.cookies_file = Path(cookies_file) if cookies_file else COOKIES_FILE
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.is_first_open = True
        self.is_logged_in = False
        self.is_first_run = True  # æ ‡è®°æ˜¯å¦é¦–æ¬¡è¿è¡Œ
    
    def load_cookies(self):
        """
        ä»æ–‡ä»¶åŠ è½½ cookies
        
        è¿”å›:
            list: cookies åˆ—è¡¨ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©ºåˆ—è¡¨
        """
        if self.cookies_file.exists():
            try:
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                print(f"âœ“ å·²åŠ è½½ä¿å­˜çš„ cookiesï¼ˆ{len(cookies)} ä¸ªï¼‰")
                return cookies
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ cookies å¤±è´¥: {e}")
                return []
        return []
    
    async def save_cookies(self):
        """
        ä¿å­˜å½“å‰çš„ cookies åˆ°æ–‡ä»¶
        """
        try:
            cookies = await self.context.cookies()
            with open(self.cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            print(f"âœ“ å·²ä¿å­˜ cookies åˆ°: {self.cookies_file}")
            print(f"  å…± {len(cookies)} ä¸ª cookies")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ cookies å¤±è´¥: {e}")
    
    async def check_login_status(self):
        """
        æ£€æµ‹å½“å‰é¡µé¢æ˜¯å¦å·²ç™»å½•
        
        è¿”å›:
            bool: Trueè¡¨ç¤ºå·²ç™»å½•ï¼ŒFalseè¡¨ç¤ºæœªç™»å½•
        """
        try:
            # æ£€æµ‹ç™»å½•çŠ¶æ€çš„å¤šç§æ–¹å¼
            # 1. æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•æ—¶æ˜¾ç¤ºï¼‰
            login_button_selectors = [
                '.c-header-login__btn',
                '.J-login-btn',
                '[class*="login-btn"]',
                'a[href*="login"]',
                '.c-login-btn',
                '.header-login',
                'text=è¯·ç™»å½•',
                'text=ç™»å½•',
            ]
            
            for selector in login_button_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        is_visible = await elem.is_visible()
                        text = await elem.inner_text() if is_visible else ''
                        # å¦‚æœæ‰¾åˆ°æ˜æ˜¾çš„ç™»å½•æŒ‰é’®ï¼Œè¯´æ˜æœªç™»å½•
                        if is_visible and ('ç™»å½•' in text or 'login' in text.lower()):
                            return False
                except:
                    continue
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æ˜µç§°æˆ–å¤´åƒï¼ˆå·²ç™»å½•æ—¶æ˜¾ç¤ºï¼‰
            logged_in_selectors = [
                '.c-header-user__name',
                '.J-user-name',
                '.user-name',
                '[class*="user-name"]',
                '[class*="nickname"]',
                '.c-header-user__avatar',
                '.user-avatar',
            ]
            
            for selector in logged_in_selectors:
                try:
                    elem = await self.page.query_selector(selector)
                    if elem:
                        is_visible = await elem.is_visible()
                        if is_visible:
                            return True
                except:
                    continue
            
            # 3. æ£€æŸ¥ cookies ä¸­æ˜¯å¦æœ‰ç™»å½•ç›¸å…³çš„ cookie
            cookies = await self.context.cookies()
            login_cookie_names = ['user_id', 'userId', 'token', 'session', 'VipUID', 'mars_cid', 'mars_sid']
            for cookie in cookies:
                if any(name.lower() in cookie.get('name', '').lower() for name in login_cookie_names):
                    if cookie.get('value'):
                        return True
            
            # 4. æ£€æŸ¥é¡µé¢URLæ˜¯å¦åŒ…å«ç™»å½•ç›¸å…³ä¿¡æ¯
            current_url = self.page.url
            if 'login' in current_url.lower() or 'signin' in current_url.lower():
                return False
            
            # é»˜è®¤è¿”å› Falseï¼Œè®©ç”¨æˆ·ç¡®è®¤
            return False
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹ç™»å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False
    
    async def wait_for_login(self):
        """
        ç­‰å¾…ç”¨æˆ·å®Œæˆç™»å½•æˆ–éªŒè¯
        æ£€æµ‹åˆ°éœ€è¦ç™»å½•æ—¶ï¼Œç­‰å¾…ç”¨æˆ·æ“ä½œå®ŒæˆåæŒ‰ Enter ç»§ç»­
        """
        print("\n" + "="*60)
        print("ğŸ” æ£€æµ‹åˆ°éœ€è¦ç™»å½•æˆ–éªŒè¯")
        print("="*60)
        print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
        print("  1. ç™»å½•æ‚¨çš„å”¯å“ä¼šè´¦å·")
        print("  2. å®Œæˆå¯èƒ½å‡ºç°çš„éªŒè¯")
        print("  3. ç¡®ä¿ç™»å½•æˆåŠŸå")
        print("-"*60)
        print(">>> å®Œæˆåè¯·æŒ‰ Enter é”®ç»§ç»­... <<<")
        print("="*60)
        
        # ç­‰å¾…ç”¨æˆ·æŒ‰ Enter
        await asyncio.get_event_loop().run_in_executor(None, input)
        
        print("\næ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        await asyncio.sleep(1)
        
        # ä¿å­˜ç™»å½•åçš„ cookies
        await self.save_cookies()
        
        # å†æ¬¡æ£€æŸ¥ç™»å½•çŠ¶æ€
        is_logged_in = await self.check_login_status()
        if is_logged_in:
            print("âœ“ ç™»å½•æˆåŠŸï¼")
            self.is_logged_in = True
        else:
            print("âš ï¸ ç™»å½•çŠ¶æ€æœªç¡®è®¤ï¼Œå°†ç»§ç»­å°è¯•...")
            # å³ä½¿æ£€æµ‹ä¸åˆ°ç™»å½•çŠ¶æ€ï¼Œä¹Ÿä¿å­˜ cookiesï¼Œç”¨æˆ·å¯èƒ½å·²ç»ç™»å½•
            self.is_logged_in = True
        
        return self.is_logged_in
    
    async def ensure_logged_in(self):
        """
        ç¡®ä¿å·²ç™»å½•çŠ¶æ€
        é¦–æ¬¡è¿è¡Œæ—¶ï¼Œæ— è®ºæ˜¯å¦æ£€æµ‹åˆ°ç™»å½•ï¼Œéƒ½ç­‰å¾…ç”¨æˆ·è°ƒè¯•å®ŒæˆåæŒ‰ Enter ç»§ç»­
        """
        # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œå§‹ç»ˆç­‰å¾…ç”¨æˆ·è°ƒè¯•
        if self.is_first_run:
            print("\n" + "="*60)
            print("ğŸ”§ é¦–æ¬¡è¿è¡Œ - è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆè°ƒè¯•")
            print("="*60)
            print("è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
            print("  1. æ£€æŸ¥é¡µé¢æ˜¯å¦æ­£å¸¸åŠ è½½")
            print("  2. å¦‚éœ€ç™»å½•ï¼Œè¯·æ‰‹åŠ¨ç™»å½•è´¦å·")
            print("  3. å®Œæˆä»»ä½•éœ€è¦çš„éªŒè¯")
            print("  4. ç¡®è®¤ä¸€åˆ‡å‡†å¤‡å°±ç»ªå")
            print("-"*60)
            print(">>> è°ƒè¯•å®Œæˆåè¯·æŒ‰ Enter é”®ç»§ç»­... <<<")
            print("="*60)
            
            # ç­‰å¾…ç”¨æˆ·æŒ‰ Enter
            await asyncio.get_event_loop().run_in_executor(None, input)
            
            print("\næ­£åœ¨ä¿å­˜çŠ¶æ€...")
            await asyncio.sleep(1)
            
            # ä¿å­˜ cookies
            await self.save_cookies()
            
            # æ ‡è®°é¦–æ¬¡è¿è¡Œå·²å®Œæˆ
            self.is_first_run = False
            self.is_logged_in = True
            
            print("âœ“ è°ƒè¯•å®Œæˆï¼Œå¼€å§‹è¿è¡Œçˆ¬è™«...")
            return True
        
        # éé¦–æ¬¡è¿è¡Œï¼Œæ£€æŸ¥ç™»å½•çŠ¶æ€
        self.is_logged_in = await self.check_login_status()
        
        if self.is_logged_in:
            print("âœ“ æ£€æµ‹åˆ°å·²ç™»å½•çŠ¶æ€")
            return True
        
        # æœªç™»å½•ï¼Œç­‰å¾…ç”¨æˆ·ç™»å½•
        return await self.wait_for_login()
        
    async def init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆä½¿ç”¨ Edgeï¼‰"""
        if not self.playwright:
            self.playwright = await async_playwright().start()
        
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            channel='msedge',
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )
        
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        )
        
        # åŠ è½½å·²ä¿å­˜çš„ cookies
        saved_cookies = self.load_cookies()
        if saved_cookies:
            try:
                await self.context.add_cookies(saved_cookies)
                print("âœ“ å·²åº”ç”¨ä¿å­˜çš„ cookies")
            except Exception as e:
                print(f"âš ï¸ åº”ç”¨ cookies å¤±è´¥: {e}")
        
        self.page = await self.context.new_page()
        
        # éšè—webdriverç‰¹å¾
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)
    
    async def check_verification(self):
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦éœ€è¦éªŒè¯
        
        è¿”å›:
            bool: Trueè¡¨ç¤ºéœ€è¦éªŒè¯ï¼ŒFalseè¡¨ç¤ºä¸éœ€è¦
        """
        try:
            current_url = self.page.url
            
            verification_keywords = ['verify', 'captcha', 'challenge', 'security', 'validate', 
                                    'éªŒè¯', 'å®‰å…¨éªŒè¯', 'äººæœºéªŒè¯', 'æ»‘å—éªŒè¯']
            if any(keyword in current_url.lower() for keyword in verification_keywords):
                return True
            
            try:
                title = await self.page.title()
                if any(keyword in title.lower() for keyword in verification_keywords):
                    return True
            except:
                pass
            
            verification_selectors = [
                'iframe[src*="captcha"]',
                'iframe[src*="verify"]',
                'iframe[src*="challenge"]',
                '.captcha',
                '.verify',
                '.challenge',
                '#captcha',
                '#verify',
                '[class*="captcha"]',
                '[class*="verify"]',
                '[class*="slider"]',
                '[id*="captcha"]',
                '[id*="verify"]',
            ]
            
            for selector in verification_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element:
                        is_visible = await element.is_visible()
                        if is_visible:
                            return True
                except:
                    continue
            
            try:
                page_text = await self.page.inner_text('body')
                verification_texts = ['å®‰å…¨éªŒè¯', 'äººæœºéªŒè¯', 'è¯·å®ŒæˆéªŒè¯', 'æ‹–åŠ¨æ»‘å—', 
                                    'éªŒè¯ç ', 'captcha', 'verification', 'challenge']
                if any(text in page_text for text in verification_texts):
                    return True
            except:
                pass
            
            return False
            
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹éªŒè¯æ—¶å‡ºé”™: {e}")
            return False
    
    async def wait_for_verification(self):
        """
        ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯
        æ£€æµ‹åˆ°éªŒè¯æ—¶ï¼Œç­‰å¾…ç”¨æˆ·æ“ä½œå®ŒæˆåæŒ‰ Enter ç»§ç»­
        """
        print("\n" + "="*60)
        print("ğŸ”’ æ£€æµ‹åˆ°éœ€è¦éªŒè¯")
        print("="*60)
        print("è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯æ“ä½œï¼š")
        print("  1. å®Œæˆæ»‘å—éªŒè¯ã€å›¾ç‰‡éªŒè¯ç­‰")
        print("  2. ç¡®ä¿éªŒè¯é€šè¿‡å")
        print("-"*60)
        print(">>> å®Œæˆåè¯·æŒ‰ Enter é”®ç»§ç»­... <<<")
        print("="*60)
        
        # ç­‰å¾…ç”¨æˆ·æŒ‰ Enter
        await asyncio.get_event_loop().run_in_executor(None, input)
        
        print("\næ­£åœ¨æ£€æŸ¥éªŒè¯çŠ¶æ€...")
        await asyncio.sleep(1)
        
        # ä¿å­˜éªŒè¯åçš„ cookies
        await self.save_cookies()
        
        # å†æ¬¡æ£€æŸ¥éªŒè¯çŠ¶æ€
        still_need_verification = await self.check_verification()
        if not still_need_verification:
            print("âœ“ éªŒè¯å·²å®Œæˆï¼")
            return True
        else:
            print("âš ï¸ ä»æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼Œè¯·å†æ¬¡å°è¯•...")
            return False
    
    async def handle_verification_with_retry(self, wait_time=127, restore_state_callback=None, skip_current=True):
        """
        å¤„ç†éªŒè¯ï¼šç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œæˆ–è€…è‡ªåŠ¨ç­‰å¾…åé‡è¯•
        
        å‚æ•°:
            wait_time: è‡ªåŠ¨ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤127ç§’
            restore_state_callback: æ¢å¤çŠ¶æ€çš„å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
            skip_current: æ˜¯å¦è·³è¿‡å½“å‰å•†å“ï¼Œé»˜è®¤True
        
        è¿”å›:
            tuple: (success: bool, should_skip: bool)
        """
        print("\n" + "="*60)
        print("âš ï¸  æ£€æµ‹åˆ°éœ€è¦éªŒè¯ï¼")
        print("="*60)
        print("è¯·é€‰æ‹©å¤„ç†æ–¹å¼ï¼š")
        print("  1. åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œç„¶åæŒ‰ Enter")
        print("  2. ç›´æ¥æŒ‰ Enter å°†è‡ªåŠ¨ç­‰å¾…åé‡æ–°æ‰“å¼€")
        print("="*60)
        
        # é¦–å…ˆå°è¯•è®©ç”¨æˆ·æ‰‹åŠ¨éªŒè¯
        verification_passed = await self.wait_for_verification()
        
        if verification_passed:
            # ç”¨æˆ·æ‰‹åŠ¨å®Œæˆäº†éªŒè¯
            if restore_state_callback:
                await restore_state_callback()
            return (True, False)  # æˆåŠŸï¼Œä¸è·³è¿‡å½“å‰å•†å“
        
        # å¦‚æœæ‰‹åŠ¨éªŒè¯å¤±è´¥ï¼Œè¯¢é—®æ˜¯å¦è‡ªåŠ¨é‡è¯•
        print("\néªŒè¯æœªé€šè¿‡ï¼Œæ˜¯å¦è‡ªåŠ¨ç­‰å¾…åé‡æ–°æ‰“å¼€ï¼Ÿ")
        print(f"  è¾“å…¥ 'y' æˆ–æŒ‰ Enter: ç­‰å¾… {wait_time} ç§’åé‡æ–°æ‰“å¼€")
        print("  è¾“å…¥ 'n': è·³è¿‡å½“å‰å•†å“")
        print("  è¾“å…¥ 'q': é€€å‡ºçˆ¬è™«")
        
        user_input = await asyncio.get_event_loop().run_in_executor(None, input)
        user_input = user_input.strip().lower()
        
        if user_input == 'q':
            print("ç”¨æˆ·é€‰æ‹©é€€å‡º...")
            raise KeyboardInterrupt("ç”¨æˆ·é€‰æ‹©é€€å‡º")
        
        if user_input == 'n':
            print("è·³è¿‡å½“å‰å•†å“...")
            return (False, True)  # å¤±è´¥ï¼Œè·³è¿‡å½“å‰å•†å“
        
        # è‡ªåŠ¨ç­‰å¾…åé‡æ–°æ‰“å¼€
        if skip_current:
            print("âš ï¸  å°†è·³è¿‡å½“å‰å•†å“ï¼Œé‡æ–°æ‰“å¼€åçˆ¬å–ä¸‹ä¸€ä¸ªå•†å“")
        print(f"å…³é—­ç½‘ç«™ï¼Œç­‰å¾… {wait_time} ç§’åé‡æ–°æ‰“å¼€...")
        print("="*60)
        
        try:
            await self.page.close()
            print("âœ“ å·²å…³é—­å½“å‰é¡µé¢")
        except:
            pass
        
        print(f"ç­‰å¾… {wait_time} ç§’...")
        for remaining in range(wait_time, 0, -1):
            print(f"  å‰©ä½™ {remaining} ç§’...", end='\r')
            await asyncio.sleep(1)
        print(f"  ç­‰å¾…å®Œæˆï¼{' '*20}")
        
        print("\né‡æ–°æ‰“å¼€ç½‘ç«™ï¼ˆå®Œå…¨é‡å¯æµè§ˆå™¨ä»¥èº²é¿éªŒè¯ï¼‰...")
        try:
            if self.page:
                try:
                    await self.page.close()
                except:
                    pass
                self.page = None
            
            if self.context:
                try:
                    await self.context.close()
                except:
                    pass
                self.context = None
            
            if self.browser:
                try:
                    await self.browser.close()
                except:
                    pass
                self.browser = None
            
            if self.playwright:
                try:
                    await self.playwright.stop()
                except:
                    pass
                self.playwright = None
            
            print("æ­£åœ¨åˆ›å»ºå…¨æ–°çš„æµè§ˆå™¨å®ä¾‹å’Œä¸Šä¸‹æ–‡...")
            await self.init_browser()
            
            url = "https://www.vip.com"
            await self.page.goto(url, wait_until='domcontentloaded', timeout=60000)
            
            await asyncio.sleep(1)
            
            print("âœ“ ç½‘ç«™å·²é‡æ–°æ‰“å¼€")
            
            self.is_first_open = True
            
            # é‡æ–°æ‰“å¼€åæ£€æŸ¥ç™»å½•çŠ¶æ€
            await self.ensure_logged_in()
            
            if await self.check_verification():
                print("âš ï¸  é‡æ–°æ‰“å¼€åä»ç„¶éœ€è¦éªŒè¯")
                return (False, skip_current)
            else:
                print("âœ“ é‡æ–°æ‰“å¼€åæ— éœ€éªŒè¯ï¼Œæ­£åœ¨æ¢å¤é¡µé¢çŠ¶æ€...")
                
                if restore_state_callback:
                    await restore_state_callback()
                
                return (True, skip_current)
                
        except Exception as e:
            print(f"âš ï¸  é‡æ–°æ‰“å¼€ç½‘ç«™æ—¶å‡ºé”™: {e}")
            return (False, skip_current)
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        # å…³é—­å‰ä¿å­˜ cookies
        if self.context:
            try:
                await self.save_cookies()
            except:
                pass
        
        if self.page:
            try:
                await self.page.close()
            except:
                pass
        if self.context:
            try:
                await self.context.close()
            except:
                pass
        if self.browser:
            try:
                await self.browser.close()
            except:
                pass
        if hasattr(self, 'playwright') and self.playwright:
            try:
                await self.playwright.stop()
            except:
                pass
    
    async def scroll_to_load(self, scroll_times=5):
        """
        æ»šåŠ¨é¡µé¢ä»¥åŠ è½½åŠ¨æ€å†…å®¹
        
        å‚æ•°:
            scroll_times: æ»šåŠ¨æ¬¡æ•°
        """
        for i in range(scroll_times):
            await self.page.evaluate('window.scrollBy(0, window.innerHeight)')
            await asyncio.sleep(0.5)
        # æ»šå›é¡¶éƒ¨
        await self.page.evaluate('window.scrollTo(0, 0)')
        await asyncio.sleep(0.3)
    
    def extract_products(self, html_content, page_num):
        """
        ä»HTMLä¸­æå–å•†å“ä¿¡æ¯ï¼ˆé’ˆå¯¹å”¯å“ä¼šé¡µé¢ç»“æ„ï¼‰
        
        HTMLç»“æ„ç¤ºä¾‹ï¼š
        <div class="c-goods-item J-goods-item c-goods-item--auto-width" data-product-id="6921691287833086801">
            <a href="//detail.vip.com/detail-1710614161-6921691287833086801.html">
                <div class="c-goods-item__img">
                    <img class="J-goods-item__img" src="//h2.appsimg.com/..." alt="å•†å“åç§°">
                </div>
                <div class="c-goods-item__sale-price J-goods-item__sale-price"><span>Â¥</span>236</div>
                <div class="c-goods-item__market-price J-goods-item__market-price"><span>Â¥</span>839</div>
                <div class="c-goods-item__discount J-goods-item__discount">2.8æŠ˜</div>
                <div class="c-goods-item__name ...">å•†å“åç§°</div>
            </a>
        </div>
        
        å‚æ•°:
            html_content: HTMLå†…å®¹
            page_num: é¡µç 
            
        è¿”å›:
            products: å•†å“åˆ—è¡¨
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        products = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å•†å“å®¹å™¨ - å¸¦æœ‰ data-product-id å±æ€§çš„ div
        product_containers = soup.find_all('div', attrs={'data-product-id': True})
        
        print(f"æ‰¾åˆ° {len(product_containers)} ä¸ªå•†å“å®¹å™¨")
        
        for idx, container in enumerate(product_containers, 1):
            try:
                product = {
                    'page': page_num,
                    'index': idx
                }
                
                # 1. æå–å•†å“IDï¼ˆä» data-product-id å±æ€§ï¼‰
                product_id = container.get('data-product-id', '')
                product['product_id'] = product_id
                
                # 2. æå–å•†å“é“¾æ¥ï¼ˆä» a æ ‡ç­¾çš„ hrefï¼‰
                link_elem = container.find('a', href=True)
                href = ''
                if link_elem:
                    href = link_elem.get('href', '')
                    if href:
                        if href.startswith('//'):
                            href = 'https:' + href
                        elif href.startswith('/'):
                            href = 'https://www.vip.com' + href
                        href = href.replace('&amp;', '&')
                product['link'] = href
                
                # 3. æå–å•†å“å›¾ç‰‡ï¼ˆä» img æ ‡ç­¾ï¼Œä¼˜å…ˆæŸ¥æ‰¾å¸¦ J-goods-item__img ç±»çš„ï¼‰
                img_elem = None
                # æ–¹æ³•1: æŸ¥æ‰¾å¸¦æœ‰ J-goods-item__img ç±»çš„ img
                for img in container.find_all('img'):
                    img_class = img.get('class', [])
                    if img_class:
                        class_str = ' '.join(img_class) if isinstance(img_class, list) else img_class
                        if 'J-goods-item__img' in class_str or 'goods-item__img' in class_str:
                            img_elem = img
                            break
                
                # æ–¹æ³•2: åœ¨ c-goods-item__img å®¹å™¨ä¸­æŸ¥æ‰¾
                if not img_elem:
                    for div in container.find_all('div'):
                        div_class = div.get('class', [])
                        if div_class:
                            class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                            if 'c-goods-item__img' in class_str:
                                img_elem = div.find('img')
                                if img_elem:
                                    break
                
                product_image = ''
                title_from_img = ''
                if img_elem:
                    img_src = (img_elem.get('src', '') or 
                              img_elem.get('data-src', '') or 
                              img_elem.get('data-original', ''))
                    if img_src:
                        if img_src.startswith('//'):
                            img_src = 'https:' + img_src
                        elif img_src.startswith('/'):
                            img_src = 'https://www.vip.com' + img_src
                        product_image = img_src
                    title_from_img = img_elem.get('alt', '')
                
                product['image'] = product_image
                
                # 4. æå–å•†å“åç§°ï¼ˆä¼˜å…ˆä» c-goods-item__nameï¼‰
                title = ''
                for div in container.find_all('div'):
                    div_class = div.get('class', [])
                    if div_class:
                        class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                        if 'c-goods-item__name' in class_str:
                            title = div.get_text(strip=True)
                            break
                if not title:
                    title = title_from_img
                
                title = ' '.join(title.split()) if title else ''
                product['title'] = title
                
                # 5. æå–å”®ä»· sale-price
                # <div class="c-goods-item__sale-price J-goods-item__sale-price"><span>Â¥</span>236</div>
                price = ''
                for div in container.find_all('div'):
                    div_class = div.get('class', [])
                    if div_class:
                        class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                        if 'c-goods-item__sale-price' in class_str or 'J-goods-item__sale-price' in class_str:
                            price_text = div.get_text(strip=True)
                            price_match = re.search(r'[\d.]+', price_text)
                            if price_match:
                                price = price_match.group()
                            break
                
                product['price'] = price
                
                # 6. æå–åŸä»· market-price
                # <div class="c-goods-item__market-price J-goods-item__market-price"><span>Â¥</span>839</div>
                original_price = ''
                for div in container.find_all('div'):
                    div_class = div.get('class', [])
                    if div_class:
                        class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                        if 'c-goods-item__market-price' in class_str or 'J-goods-item__market-price' in class_str:
                            market_price_text = div.get_text(strip=True)
                            price_match = re.search(r'[\d.]+', market_price_text)
                            if price_match:
                                original_price = price_match.group()
                            break
                
                product['original_price'] = original_price
                
                # 7. æå–æŠ˜æ‰£
                # <div class="c-goods-item__discount J-goods-item__discount">2.8æŠ˜</div>
                discount = ''
                for div in container.find_all('div'):
                    div_class = div.get('class', [])
                    if div_class:
                        class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                        if 'c-goods-item__discount' in class_str or 'J-goods-item__discount' in class_str:
                            discount = div.get_text(strip=True)
                            break
                
                product['discount'] = discount
                
                # 8. æå–å“ç‰Œä¿¡æ¯
                brand = ''
                for div in container.find_all('div'):
                    div_class = div.get('class', [])
                    if div_class:
                        class_str = ' '.join(div_class) if isinstance(div_class, list) else div_class
                        if 'c-goods-item__brand' in class_str and 'logo' not in class_str:
                            brand = div.get_text(strip=True)
                            break
                
                product['brand'] = brand
                
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå•†å“ï¼ˆå¿…é¡»æœ‰æ ‡é¢˜å’Œé“¾æ¥ï¼‰
                is_valid = bool(product.get('title') and product.get('link'))
                
                if is_valid:
                    products.append(product)
                    title_preview = product['title'][:40] + '...' if len(product['title']) > 40 else product['title']
                    price_display = f"Â¥{product['price']}" if product.get('price') else 'N/A'
                    discount_display = f" ({product['discount']})" if product.get('discount') else ''
                    print(f"å•†å“ {len(products)}: {title_preview} - {price_display}{discount_display}")
                
            except Exception as e:
                print(f"æå–å•†å“ {idx} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print(f"\næ€»å…±æå–åˆ° {len(products)} ä¸ªæœ‰æ•ˆå•†å“")
        return products


async def crawl_products_automated(products, num_pages_per_product, headless=False, save_html=False, output_dir='vips_data'):
    """
    æŒ‰ç…§è‡ªåŠ¨åŒ–æµç¨‹çˆ¬å–å¤šä¸ªå•†å“çš„å¤šé¡µæ•°æ®
    
    å‚æ•°:
        products: å•†å“åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ['æ‰‹æœº', 'è¡£æœ', 'ç”µè„‘']
        num_pages_per_product: æ¯ä¸ªå•†å“è¦çˆ¬å–çš„é¡µæ•°
        headless: æ˜¯å¦æ— å¤´æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰
        save_html: æ˜¯å¦ä¿å­˜HTMLæ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
    
    è¿”å›:
        all_products: æ‰€æœ‰å•†å“åˆ—è¡¨
    """
    # åæ ‡é…ç½®ï¼ˆéœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´ï¼‰
    # å”¯å“ä¼šé¦–é¡µæœç´¢æ¡†ä½ç½® - éœ€è¦æ ¹æ®å®é™…å±å¹•åˆ†è¾¨ç‡è°ƒæ•´
    SEARCH_BAR_X, SEARCH_BAR_Y = 960, 80  # æœç´¢æ ï¼ˆé¡µé¢é¡¶éƒ¨ä¸­é—´ï¼‰
    SEARCH_BUTTON_X, SEARCH_BUTTON_Y = 1100, 80  # æœç´¢æŒ‰é’®
    NEXT_PAGE_X, NEXT_PAGE_Y = 960, 900  # ä¸‹ä¸€é¡µæŒ‰é’®ï¼ˆé¡µé¢åº•éƒ¨ï¼‰
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    crawler = VipsCrawler(headless=headless, save_html=save_html)
    all_products = []
    
    try:
        await crawler.init_browser()
        
        # æ‰“å¼€é¦–é¡µ
        url = "https://www.vip.com"
        print(f"\n{'='*60}")
        print(f"æ‰“å¼€ç½‘é¡µ: {url}")
        print(f"{'='*60}")
        await crawler.page.goto(url, wait_until='domcontentloaded', timeout=60000)
        # ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½
        await asyncio.sleep(3)
        
        # æ£€æµ‹ç™»å½•çŠ¶æ€ï¼Œå¦‚æœæœªç™»å½•åˆ™ç­‰å¾…ç”¨æˆ·ç™»å½•
        print("\næ£€æµ‹ç™»å½•çŠ¶æ€...")
        await crawler.ensure_logged_in()
        
        # éªŒè¯é‡è¯•æœºåˆ¶å˜é‡
        current_wait_time = 127
        consecutive_failures = 0
        skip_first_product = False
        
        # å®šä¹‰æ¢å¤é¡µé¢çŠ¶æ€çš„å‡½æ•°
        async def restore_page_state():
            """æ¢å¤é¡µé¢çŠ¶æ€"""
            print("\næ¢å¤é¡µé¢çŠ¶æ€...")
            await asyncio.sleep(0.5)
            print("âœ“ é¡µé¢å·²æ¢å¤")
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦éªŒè¯
        while await crawler.check_verification():
            success, _ = await crawler.handle_verification_with_retry(
                current_wait_time, 
                restore_state_callback=restore_page_state
            )
            
            if success:
                current_wait_time = 127
                consecutive_failures = 0
                break
            else:
                consecutive_failures += 1
                current_wait_time = 127 + (consecutive_failures * 10)
                print(f"è¿ç»­å¤±è´¥ {consecutive_failures} æ¬¡ï¼Œä¸‹æ¬¡ç­‰å¾…æ—¶é—´: {current_wait_time} ç§’")
        
        # æ ‡è®°é¦–æ¬¡æ‰“å¼€å·²å®Œæˆ
        crawler.is_first_open = False
        
        # éå†æ¯ä¸ªå•†å“
        for product_idx, product_name in enumerate(products, 1):
            if skip_first_product and product_idx == 1:
                print(f"\n{'='*60}")
                print(f"âš ï¸  è·³è¿‡å•†å“ {product_idx}/{len(products)}: {product_name}ï¼ˆå› éªŒè¯ä¸­æ–­ï¼‰")
                print(f"{'='*60}")
                continue
            
            print(f"\n{'='*60}")
            print(f"å•†å“ {product_idx}/{len(products)}: {product_name}")
            print(f"{'='*60}")
            
            product_products = []
            should_skip = False
            
            # ä½¿ç”¨URLç›´æ¥æœç´¢ï¼ˆæ›´å¯é çš„æ–¹å¼ï¼‰
            search_url = f"https://category.vip.com/suggest.php?keyword={product_name}&ff=search|home|head|input"
            print(f"\næ‰“å¼€æœç´¢é¡µé¢: {search_url}")
            
            try:
                await crawler.page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
                await asyncio.sleep(2)
                
                # æ£€æµ‹æ˜¯å¦éœ€è¦éªŒè¯
                while await crawler.check_verification():
                    success, should_skip = await crawler.handle_verification_with_retry(
                        current_wait_time,
                        restore_state_callback=restore_page_state,
                        skip_current=True
                    )
                    if success:
                        current_wait_time = 127
                        consecutive_failures = 0
                        if should_skip:
                            print(f"âš ï¸  è·³è¿‡å½“å‰å•†å“ {product_name}ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå•†å“")
                            break
                        break
                    else:
                        consecutive_failures += 1
                        current_wait_time = 127 + (consecutive_failures * 10)
                
                if should_skip:
                    continue
                
            except Exception as e:
                print(f"âš ï¸ æ‰“å¼€æœç´¢é¡µé¢å¤±è´¥: {e}")
                continue
            
            # éå†æ¯ä¸ªé¡µé¢
            for page_num in range(1, num_pages_per_product + 1):
                print(f"\n  {'-'*50}")
                print(f"  ç¬¬ {page_num}/{num_pages_per_product} é¡µ")
                print(f"  {'-'*50}")
                
                try:
                    # æ£€æµ‹éªŒè¯
                    while await crawler.check_verification():
                        success, should_skip = await crawler.handle_verification_with_retry(
                            current_wait_time,
                            restore_state_callback=restore_page_state,
                            skip_current=True
                        )
                        if success:
                            current_wait_time = 127
                            consecutive_failures = 0
                            if should_skip:
                                break
                            break
                        else:
                            consecutive_failures += 1
                            current_wait_time = 127 + (consecutive_failures * 10)
                    
                    if should_skip:
                        break
                    
                    # ç­‰å¾…é¡µé¢ç¨³å®š
                    await asyncio.sleep(1)
                    try:
                        await crawler.page.wait_for_load_state('domcontentloaded', timeout=15000)
                    except:
                        pass
                    
                    # æ»šåŠ¨åŠ è½½åŠ¨æ€å†…å®¹
                    print("  æ»šåŠ¨é¡µé¢åŠ è½½å•†å“...")
                    await crawler.scroll_to_load(scroll_times=5)
                    
                    # è·å–HTMLå†…å®¹
                    max_retries = 3
                    html_content = None
                    for attempt in range(max_retries):
                        try:
                            html_content = await crawler.page.content()
                            break
                        except Exception as e:
                            if attempt < max_retries - 1:
                                await asyncio.sleep(0.5)
                            else:
                                raise
                    
                    if html_content:
                        # ä¿å­˜HTML
                        if save_html:
                            html_file = os.path.join(output_dir, f"{product_name}_page_{page_num}.html")
                            with open(html_file, 'w', encoding='utf-8') as f:
                                f.write(html_content)
                            print(f"  âœ“ HTMLå·²ä¿å­˜: {html_file}")
                        
                        # æå–å•†å“ä¿¡æ¯
                        products_data = crawler.extract_products(html_content, page_num)
                        product_products.extend(products_data)
                        print(f"  âœ“ ç¬¬ {page_num} é¡µå®Œæˆï¼Œæå–åˆ° {len(products_data)} ä¸ªå•†å“")
                        
                        consecutive_failures = 0
                        current_wait_time = 127
                    else:
                        print(f"  âš ï¸ ç¬¬ {page_num} é¡µæ— æ³•è·å–HTMLå†…å®¹")
                
                except Exception as e:
                    print(f"  âš ï¸ ç¬¬ {page_num} é¡µçˆ¬å–å‡ºé”™: {e}")
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€é¡µï¼Œç‚¹å‡»ä¸‹ä¸€é¡µ
                if page_num < num_pages_per_product:
                    print(f"  ç‚¹å‡»ä¸‹ä¸€é¡µ...")
                    try:
                        # å°è¯•ä½¿ç”¨é€‰æ‹©å™¨ç‚¹å‡»ä¸‹ä¸€é¡µ
                        next_page_selectors = [
                            '.J-page-item.page-next-txt',
                            '.c-page__item--next',
                            'a.page-next',
                            '[class*="next"]',
                            '//a[contains(text(),"ä¸‹ä¸€é¡µ")]',
                        ]
                        
                        clicked = False
                        for selector in next_page_selectors:
                            try:
                                if selector.startswith('//'):
                                    # XPath
                                    elem = await crawler.page.query_selector(f'xpath={selector}')
                                else:
                                    elem = await crawler.page.query_selector(selector)
                                
                                if elem:
                                    await elem.click()
                                    clicked = True
                                    print("  âœ“ å·²ç‚¹å‡»ä¸‹ä¸€é¡µ")
                                    break
                            except:
                                continue
                        
                        if not clicked:
                            # å°è¯•é€šè¿‡URLç¿»é¡µ
                            current_url = crawler.page.url
                            if 'page=' in current_url:
                                new_url = re.sub(r'page=\d+', f'page={page_num + 1}', current_url)
                            else:
                                separator = '&' if '?' in current_url else '?'
                                new_url = f"{current_url}{separator}page={page_num + 1}"
                            
                            await crawler.page.goto(new_url, wait_until='domcontentloaded', timeout=60000)
                            print(f"  âœ“ é€šè¿‡URLè·³è½¬åˆ°ç¬¬ {page_num + 1} é¡µ")
                        
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        print(f"  âš ï¸ ç¿»é¡µå¤±è´¥: {e}")
                        break
            
            # ä¿å­˜å½“å‰å•†å“çš„æ•°æ®
            if product_products:
                all_products.extend(product_products)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                
                # ä¿å­˜JSON
                json_file = os.path.join(output_dir, f"{product_name}_products_{timestamp}.json")
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(product_products, f, ensure_ascii=False, indent=2)
                print(f"\nâœ“ {product_name} çš„JSONæ•°æ®å·²ä¿å­˜: {json_file}")
                
                print(f"âœ“ {product_name} å®Œæˆï¼Œå…±æå– {len(product_products)} ä¸ªå•†å“")
            else:
                print(f"\nâš ï¸ {product_name} æœªæå–åˆ°ä»»ä½•å•†å“")
        
        # ä¿å­˜æ‰€æœ‰å•†å“çš„æ€»æ•°æ®
        if all_products:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            all_json_file = os.path.join(output_dir, f"all_products_{timestamp}.json")
            with open(all_json_file, 'w', encoding='utf-8') as f:
                json.dump(all_products, f, ensure_ascii=False, indent=2)
            print(f"\n{'='*60}")
            print(f"âœ“ æ‰€æœ‰å•†å“æ•°æ®å·²ä¿å­˜: {all_json_file}")
            print(f"æ€»å…±çˆ¬å–åˆ° {len(all_products)} ä¸ªå•†å“")
            print(f"{'='*60}")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await crawler.close()
    
    return all_products


def get_crawled_products(data_dir='vips_data', check_html=True):
    """
    ä»æ•°æ®ç›®å½•ä¸­æå–å·²çˆ¬å–çš„å•†å“åç§°
    
    å‚æ•°:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        check_html: æ˜¯å¦ä¹Ÿæ£€æŸ¥ HTML æ–‡ä»¶
    
    è¿”å›:
        set: å·²çˆ¬å–çš„å•†å“åç§°é›†åˆ
    """
    from pathlib import Path
    
    crawled_products = set()
    data_path = Path(data_dir)
    
    if not data_path.exists():
        return crawled_products
    
    # æŸ¥æ‰¾æ‰€æœ‰ *_products_*.json æ–‡ä»¶
    for json_file in data_path.glob('*_products_*.json'):
        if json_file.name.startswith('all_products'):
            continue
        match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
        if match:
            product_name = match.group(1)
            crawled_products.add(product_name)
    
    if check_html:
        for html_file in data_path.glob('*_page_*.html'):
            match = re.match(r'^(.+?)_page_\d+\.html$', html_file.name)
            if match:
                product_name = match.group(1)
                crawled_products.add(product_name)
    
    return crawled_products


def filter_products(products_list, crawled_products):
    """
    ä»å•†å“åˆ—è¡¨ä¸­ç§»é™¤å·²çˆ¬å–çš„å•†å“
    
    å‚æ•°:
        products_list: åŸå§‹å•†å“åˆ—è¡¨
        crawled_products: å·²çˆ¬å–çš„å•†å“é›†åˆ
    
    è¿”å›:
        tuple: (æœªçˆ¬å–çš„å•†å“åˆ—è¡¨, å·²çˆ¬å–çš„å•†å“åˆ—è¡¨)
    """
    uncrawled = []
    crawled = []
    
    for product in products_list:
        if product in crawled_products:
            crawled.append(product)
        else:
            uncrawled.append(product)
    
    return uncrawled, crawled


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("å”¯å“ä¼š VIP.com å•†å“çˆ¬è™«")
    print("="*60)
    
    # å•†å“åˆ—è¡¨
    products_list = ['è€³ç½©', 'ç‡•å°¾æœ', 'ç¯ä¿åŒ…è¢‹']
    num_pages = 20  # æ¯ä¸ªå•†å“çˆ¬å–çš„é¡µæ•°
    
    # è‡ªåŠ¨æ£€æŸ¥å¹¶è¿‡æ»¤å·²çˆ¬å–çš„å•†å“
    # crawled_products = get_crawled_products('vips_data', check_html=True)
    # print(f"\nå·²çˆ¬å–çš„å•†å“ ({len(crawled_products)} ä¸ª):")
    # for product in sorted(crawled_products):
    #    print(f"  - {product}")
    
    # products_list, _ = filter_products(products_list, crawled_products)
    
    # print(f"\nè¿‡æ»¤åå¾…çˆ¬å–çš„å•†å“ ({len(products_list)} ä¸ª):")
    # for product in sorted(products_list):
    #    print(f"  - {product}")
    
    if not products_list:
        print("\næ‰€æœ‰å•†å“å·²çˆ¬å–å®Œæˆï¼Œæ— éœ€å†æ¬¡è¿è¡Œã€‚")
    else:
        print("\nå¼€å§‹çˆ¬å–å‰©ä½™å•†å“...")
        all_products = asyncio.run(crawl_products_automated(
            products=products_list,
            num_pages_per_product=num_pages,
            headless=False,
            save_html=True,
            output_dir='vips_data'
        ))
        
        print(f"\nçˆ¬å–å®Œæˆï¼å…±è·å– {len(all_products)} ä¸ªå•†å“")
