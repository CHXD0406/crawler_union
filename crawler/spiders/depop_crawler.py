# æ–‡ä»¶å: depop_crawler.py
import asyncio
import json
import os
import time
from datetime import datetime
from bs4 import BeautifulSoup
from crawler_base import BaseCrawler, MultiCrawlerManager
import re

class DepopCrawler(BaseCrawler):
    def extract_products(self, html_content, skip_count=0):
        """
        Depop ä¸“å±çš„ HTML è§£æé€»è¾‘
        """
        soup = BeautifulSoup(html_content, 'html.parser')
        new_products = []
        all_containers = soup.select('li[class*="styles_listItem"]')

        containers_to_process = all_containers[skip_count:]
        if not containers_to_process: return []

        print(f"ğŸ” [Port {self.port}] è§£ææ–°å¢æ•°æ®: {len(containers_to_process)} æ¡...")

        local_index = skip_count
        for container in containers_to_process:
            try:
                local_index += 1
                product = {'index': local_index}

                # æå–å›¾ç‰‡
                img_tag = container.select_one('img[class*="_mainImage"]')
                if not img_tag: img_tag = container.select_one('img[src]')
                img_src = ""
                if img_tag:
                    img_src = img_tag.get('src') or img_tag.get('data-src') or ""
                    if not img_src and img_tag.get('srcset'):
                        img_src = img_tag.get('srcset').split(' ')[0]
                product['image'] = img_src

                # æå–ä»·æ ¼
                price_tag = container.select_one('p[aria-label="Discounted price"]')
                if not price_tag: price_tag = container.select_one('p[aria-label="Price"]')
                if not price_tag: price_tag = container.select_one('p[aria-label="Full price"]')
                product['price'] = price_tag.get_text(strip=True) if price_tag else "0"

                # æå–é“¾æ¥/æ ‡é¢˜
                link_tag = container.select_one('a[class*="styles_unstyledLink"]')
                href, merchant, title = "", "", ""
                if link_tag:
                    href = link_tag.get('href', '')
                    if href.startswith('/'): href = 'https://www.depop.com' + href
                    try:
                        clean_path = href.split('?')[0].strip('/')
                        if 'products/' in clean_path: clean_path = clean_path.split('products/')[-1]
                        parts = clean_path.split('-')
                        if len(parts) >= 2:
                            merchant = parts[0]
                            title = " ".join(parts[1:-1]).capitalize() if len(parts) > 2 else parts[1]
                        else:
                            title = clean_path
                    except:
                        title = "Parse Error"

                product['link'] = href;
                product['title'] = title;
                product['seller'] = merchant
                if product['link']: new_products.append(product)

            except Exception as e:
                continue
        return new_products

    async def crawl(self, tasks, max_count, output_dir):
        """
        Depop ä¸“å±çš„çˆ¬å–å¾ªç¯é€»è¾‘ (è¦†ç›–çˆ¶ç±»æ–¹æ³•)
        """
        # æŒ‰è¿›åº¦æ’åº
        tasks.sort(key=lambda x: x[1])

        try:
            # 1. è°ƒç”¨çˆ¶ç±»æ–¹æ³•å¯åŠ¨æµè§ˆå™¨
            await self.init_browser()

            if not self.page:
                print(f"âŒ [Port {self.port}] æµè§ˆå™¨æœªå°±ç»ª")
                return

            # 2. è°ƒæ•´ç¼©æ”¾ (Depop ä¸“å±ä¼˜åŒ–)
            try:
                await self.page.evaluate("document.body.style.zoom = '0.3'")
            except:
                pass

            # 3. å¼€å§‹éå†ä»»åŠ¡
            for product_name, start_index in tasks:
                print(f"\n{'=' * 40}\n[Port {self.port}] æ­£åœ¨çˆ¬å–: {product_name} (Index {start_index})\n{'=' * 40}")

                # æ„é€ æœç´¢URL
                search_query = product_name.strip().replace(' ', '+')
                search_url = f"https://www.depop.com/search/?q={search_query}"

                try:
                    await self.page.goto(search_url)
                    try:
                        await self.page.wait_for_load_state('networkidle', timeout=15000)
                    except:
                        pass
                    await asyncio.sleep(2)
                except Exception as e:
                    print(f"âŒ [Port {self.port}] é¡µé¢è·³è½¬å¤±è´¥: {e}")
                    continue

                # --- æ™ºèƒ½æ— é™æ»šåŠ¨ (Depop éœ€è¦) ---
                current_count = 0
                retry_count = 0
                item_selector = 'li[class*="styles_listItem"]'

                while current_count < max_count:
                    await self.page.keyboard.press("End")
                    await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

                    # ç­‰å¾…æ–°å…ƒç´ å‡ºç°
                    try:
                        await self.page.wait_for_function(
                            f"document.querySelectorAll('{item_selector}').length > {current_count}",
                            timeout=20000
                        )
                        await asyncio.sleep(2)
                    except:
                        pass

                    try:
                        new_count = await self.page.evaluate(f"document.querySelectorAll('{item_selector}').length")
                    except:
                        new_count = current_count

                    if new_count > current_count:
                        current_count = new_count
                        retry_count = 0
                        print(f"  ğŸ“‰ [Port {self.port}] æ»šåŠ¨åŠ è½½ä¸­... (å½“å‰: {current_count})", end='\r')
                    else:
                        retry_count += 1
                        print(f"  âš ï¸ [Port {self.port}] æ— æ–°å†…å®¹ ({retry_count}/5)...")
                        await self.page.evaluate("window.scrollBy(0, -500)")
                        await asyncio.sleep(2)
                        await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                        if retry_count >= 5: break

                # --- æå–ä¸ä¿å­˜ ---
                print(f"\n[Port {self.port}] æå–æ•°æ®...")
                try:
                    html = await self.page.content()
                    data = self.extract_products(html, skip_count=start_index)
                    if data:
                        self._save_data(product_name, data, start_index, output_dir)
                        print(f"  âœ“ [Port {self.port}] ä¿å­˜æˆåŠŸ: {len(data)} æ¡")
                    else:
                        print(f"  âš ï¸ [Port {self.port}] æœªæå–åˆ°æ•°æ®")
                except Exception as e:
                    print(f"  âŒ [Port {self.port}] å¤„ç†å¤±è´¥: {e}")

        except Exception as e:
            print(f"âŒ [Port {self.port}] è¿›ç¨‹å´©æºƒ: {e}")
        finally:
            await self.close()  # è°ƒç”¨çˆ¶ç±»æ¸…ç†

    def _save_data(self, product_name, new_data, start_index, output_dir):
        """ä¿å­˜æ•°æ®è¾…åŠ©å‡½æ•°"""
        final_data = new_data

        print(f"ğŸ“Š å‡†å¤‡ä¿å­˜ {len(final_data)} æ¡æ•°æ®...")


        files_to_remove = []

        if start_index > 0:
            print(f"\nğŸ”„ [åˆå¹¶æ¨¡å¼] æ£€æµ‹åˆ°ç»­ä¼  (èµ·å§‹ Index {start_index})ï¼Œæ£€ç´¢æ—§æ–‡ä»¶...")
            try:
                from pathlib import Path
                data_path = Path(output_dir)
                candidate_files = []
                for f in data_path.glob('*_products_*.json'):
                    match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', f.name)
                    if match and match.group(1) == product_name:
                        candidate_files.append(f)
                candidate_files.sort(key=lambda x: x.name, reverse=True)

                if candidate_files:
                    latest_json = candidate_files[0]
                    with open(latest_json, 'r', encoding='utf-8') as f:
                        old_data = json.load(f)

                    if isinstance(old_data, list):
                        if len(old_data) != start_index:
                            print(
                                f"   âš ï¸ è­¦å‘Š: æ—§æ•°æ®é•¿åº¦ ({len(old_data)}) ä¸ start_index ({start_index}) ä¸ä¸€è‡´")

                        final_data = old_data + final_data
                        print(
                            f"   â• åˆå¹¶æˆåŠŸ: æ—§({len(old_data)}) + æ–°({len(final_data )}) = æ€»({len(final_data)})")
                        files_to_remove.append(latest_json)
                        old_csv = latest_json.with_suffix('.csv')
                        if old_csv.exists(): files_to_remove.append(old_csv)
                else:
                    print("   âš ï¸ æœªæ‰¾åˆ°æ—§æ–‡ä»¶")
            except Exception as e:
                print(f"   âŒ åˆå¹¶å‡ºé”™: {e}")

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        new_json_name = os.path.join(output_dir, f"{product_name}_products_{timestamp}.json")
        new_csv_name = os.path.join(output_dir, f"{product_name}_products_{timestamp}.csv")

        if not os.path.exists(output_dir): os.makedirs(output_dir)

        with open(new_json_name, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONä¿å­˜: {os.path.basename(new_json_name)}")

        import csv
        with open(new_csv_name, 'w', encoding='utf-8', newline='') as f:
            if final_data:
                keys = final_data[0].keys()
                writer = csv.DictWriter(f, fieldnames=keys)
                writer.writeheader()
                writer.writerows(final_data)
        print(f"ğŸ’¾ CSVä¿å­˜: {os.path.basename(new_csv_name)}")

        if files_to_remove:
            print(f"ğŸ§¹ æ¸…ç†æ—§ç‰ˆæœ¬æ–‡ä»¶...")
            for f in files_to_remove:
                try:
                    os.remove(f);
                    print(f"   ğŸ—‘ï¸ åˆ é™¤: {f.name}")
                except:
                    pass


# ==================== æ ¸å¿ƒå·¥å…·: ä»»åŠ¡è·å–ä¸æ–­ç‚¹æ£€æµ‹ ====================
def get_tasks_from_file(name_file, max_count, data_dir):
    """
    è¯»å–ä»»åŠ¡åˆ—è¡¨ï¼Œå¹¶æ‰«ææ•°æ®ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å·²çˆ¬å–çš„è¿›åº¦ã€‚
    è¿”å›æ ¼å¼: [(product_name, start_index), ...]
    """
    import json
    from pathlib import Path

    # 1. è¯»å–åŸå§‹ä»»åŠ¡åˆ—è¡¨
    try:
        if not os.path.exists(name_file):
            print(f"âŒ æœªæ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶: {name_file}")
            return []
        with open(name_file, 'r', encoding='utf-8') as f:
            names = json.load(f)
        # å»é‡
        product_names = list(set(names))
    except Exception as e:
        print(f"âŒ è¯»å–ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
        return []

    # 2. æ‰«æç°æœ‰çš„ JSON æ–‡ä»¶ï¼Œè·å–è¿›åº¦
    tasks_progress = {name: 0 for name in product_names}
    data_path = Path(data_dir)

    if data_path.exists():
        print(f"ğŸ” æ­£åœ¨æ‰«æ {data_dir} ç›®å½•ä¸‹çš„æ–­ç‚¹ä¿¡æ¯...")
        for json_file in data_path.glob('*_products_*.json'):
            # æ’é™¤æ±‡æ€»æ–‡ä»¶
            if json_file.name.startswith('all_products'): continue

            # è§£ææ–‡ä»¶å: name_products_timestamp.json
            match = re.match(r'^(.+?)_products_\d{8}_\d{6}\.json$', json_file.name)
            if not match: continue

            p_name = match.group(1)

            # å¦‚æœè¿™ä¸ªå•†å“åœ¨æˆ‘ä»¬çš„ä»»åŠ¡åˆ—è¡¨ä¸­
            if p_name in tasks_progress:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)

                    if data and isinstance(data, list):
                        # è·å–æœ€åä¸€æ¡æ•°æ®çš„ index ä½œä¸ºå½“å‰è¿›åº¦
                        # å‡è®¾æ¯æ¡æ•°æ®éƒ½æœ‰ 'index' å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åˆ—è¡¨é•¿åº¦
                        last_item = data[-1]
                        current_index = int(last_item.get('index', len(data)))

                        # æ›´æ–°æœ€å¤§è¿›åº¦ï¼ˆé˜²æ­¢æœ‰å¤šä¸ªæ—§æ–‡ä»¶ï¼Œå–æœ€å¤§çš„é‚£ä¸ªï¼‰
                        if current_index > tasks_progress[p_name]:
                            tasks_progress[p_name] = current_index
                except Exception as e:
                    print(f"  âš ï¸ è¯»å–æ–‡ä»¶ {json_file.name} å¤±è´¥: {e}")
                    continue

    # 3. ç”Ÿæˆæœ€ç»ˆä»»åŠ¡åˆ—è¡¨
    final_tasks = []
    for name, progress in tasks_progress.items():
        if progress < max_count:
            if progress > 0:
                print(f"  ğŸ”„ æ¢å¤ä»»åŠ¡: {name} (ä» {progress} å¼€å§‹)")
            final_tasks.append((name, progress))
        else:
            # print(f"  âœ… è·³è¿‡å·²å®Œæˆ: {name}") # å¯é€‰ï¼šæ‰“å°å·²å®Œæˆçš„ä»»åŠ¡
            pass

    # æŒ‰åç§°æ’åºï¼Œä¿è¯æ¯æ¬¡è¿è¡Œé¡ºåºä¸€è‡´
    return sorted(final_tasks, key=lambda x: x[0])



# ==================== ä¸»å…¥å£ ====================
if __name__ == "__main__":
    print("Depop çˆ¬è™« (åŸºäº CrawlerBase)")
    print("=" * 60)

    # 1. é…ç½®å‚æ•°
    WORKER_COUNT = 4
    BASE_PORT = 9222
    MAX_CRAWL = 400
    OUTPUT_DIR = 'depop_data'

    # 2. è·å–ä»»åŠ¡
    all_tasks = get_tasks_from_file('clothing_leaf_names.json', MAX_CRAWL)

    if all_tasks:
        print(f"ğŸ“¦ ä»»åŠ¡æ€»æ•°: {len(all_tasks)}")

        # 3. åˆå§‹åŒ–é€šç”¨ç®¡ç†å™¨ï¼Œä¼ å…¥ DepopCrawler ç±»
        manager = MultiCrawlerManager(
            crawler_class=DepopCrawler,
            base_port=BASE_PORT,
            workers=WORKER_COUNT
        )

        # 4. è¿è¡Œ
        asyncio.run(manager.run(all_tasks, MAX_CRAWL, OUTPUT_DIR))
    else:
        print("æœªæ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶æˆ–ä»»åŠ¡ä¸ºç©º")
